\label{research}
So far, we discussed an extension of the pilot abstraction to support data analytics and task based data intensive applications on HPC resources, a comparison between different task-based data oriented frameworks, and a design comparison for scientific workflows.
In addition, we identified a need for the execution of scientific computational campaigns with minimum user intervention, independent from users' scientific domain.
In this section, we motivate and propose a Campaign Manager (CM) for creating and enacting a campaign execution plan.


\subsection{Proposed Topic}
Campaigns from the biomolecular and earth sciences are diverse in terms of composition, number and size of workflow members, and dynamicity.
Biomolecular science campaigns may be comprised from a small number of workflows with millions of tasks, or thousands of workflows with tens to hundreds tasks~\cite{dakka2018high}. 
Earth sciences campaigns, especially those which use VHR satellite imagery, comprise of workflows with thousands of tasks.
The number of workflows depends on the number of images the user has access to as well as the time they are able to obtain imagery.
These workflows can be static~\cite{paraskevakos2019workflow} or dynamic~\cite{dakka2018high}.

% ------------------------------------------------------------------------------
% Scientific campaings
Scientific workflows are generally described by a direct acyclic graph (DAG), where the nodes are tasks and the edges dependencies.
A subset of this general description of workflows are those which can be represented via the Pipeline, Stage, Task (PST) model~\cite{balasubramanian2018harnessing}.
The PST model describes workflows as a set of pipelines, where each pipeline is a sequence of stages.
Each stage then is a set of tasks that need to be executed.
Concurrency is achieved in the level of pipelines and the level of tasks. 
Based on our use cases, we particularly interested in scientific campaigns comprised by workflows that can be described via the PST model.
Figure~\ref{fig:bio_earth_workflows} shows two example workflows from the biomolecular sciences (Fig.~\ref{fig:bio_workflow}) and the earth sciences (Fig.~\ref{fig:earth_workflow}). 

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/bio_workflow.pdf}
        \caption{}
        \label{fig:bio_workflow}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/earth_workflow.pdf}
        \caption{}
        \label{fig:earth_workflow}
    \end{subfigure}
    \caption{Biomolecular and Earth Science example workflows. \ref{fig:bio_workflow} Biomolecular workflow based on PST model example~\cite{dakka2018concurrent}; \ref{fig:earth_workflow} Earth science workflow based on PST model example~\cite{paraskevakos2019workflow}}\label{fig:bio_earth_workflows}
\end{figure*}

Resource requirements of these campaigns are heterogeneous, in terms of number of resources required and type. 
Biomolecular science software tools support MPI/OpenMP, GPUs, and other accelerators, such as Intel Phi processors~\cite{cheatham2015impact} for executing either simulations or analysis.
In addition, some biomolecular analysis tool require data oriented frameworks, such as PMDA~\cite{fan2019pmda}.
Earth sciences applications, based on imagery, are creating workflows that are executing a series of CPU based preprocessing, and eventually execute a computer vision algorithm on GPUs~\cite{paraskevakos2019workflow}.

HPC resource are dynamic in nature, as their availability is subject to a broad spectrum of policies.
These policies determine:
\begin{inparaenum}[1)]
\item the maximum number of resources and walltime a user can request concurrently in a single request,
\item the maximum number of concurrently submitted or active requests, and
\item one or more usage charging models, which determine the total amount or resources the has access to.
\end{inparaenum}

The makespan of a campaign can be from a simple arithmetics calculation to a complex problem, depending on the heterogeneity of the campaign's workflows, maximum concurrency and resource dynamicity.
Workflows can be heterogeneous in space, i.e. the number of tasks and resource requirements, and in time, i.e. the execution time needed to execute.
When workflows are the same in space and time, they are considered homogeneous.
Assuming enough resources to execute a workflow, when a campaign consists of homogeneous workflows any order of workflow execution would provide the same makespan. 
When heterogeneity is introduced, randomly ordering the workflows' execution does not provide the same makespan for different instances of the ordering.
This is especially true, when there are not enough resources to execute all the workflows concurrently.
As a result, calculating the makespan of the campaign based on workflow order execution becomes important to decide whether the selected mapping satisfies or not the campaign's computational campaign.

% ----------------------------------------------------------------------------
% campaign makespan modeling
We propose to initially utilize and extend the Heterogeneous Earliest Finish Time (HEFT)~\cite{topcuoglu2002performance} algorithm.
HEFT is an offline scheduling algorithm which calculates the makespan of a workflow on heterogeneous resources.
HEFT uses a matrix to represent execution time of tasks on resources, assigning tasks to the resource that minimizes the finish time of the task, and has complexity proportional to the number of dependencies between tasks and the number of resources offered.
Furthermore, there has been some initial research to extend HEFT to resources that provide CPU and GPUs~\cite{shetti2013optimization}, as well as a HEFT extension on dynamic resources~\cite{dong2007pfas}.

There are several alternative methods and algorithms to calculate and optimize the makespan of a workflow~\cite{lu2019review}, including queuing networks~\cite{yao2019throughput,bao2019performance}, domain specific languages~\cite{carothers2017durango,maheshwari2016workflow}, and machine learning~\cite{witt2019predictive,pumma2017runtime}.
Queuing networks will be of limited use because they require from the user to provide a queuing network equivalent of the campaign.
As a result, having the user provide a queuing network representation of the campaign adds an additional layer of complexity.
Domain specific languages would require too much engineering effort to convert a workflow representation based on domain specific assumptions, e.g. MPI style workflow, or specific languages representation, e.g Swift, to a PST model representation.
Machine Learning approaches would require model training, validation and testing to produce a model.
In addition, since the execution is done on dynamic resources, the model should be retrained after every workflow execution.

% ----------------------------------------------------------------------------
% Initial Assumptions
We propose to extend HEFT to support execution of a computational campaign in dynamic resources.
When executing a workflows HEFT assumes that any available resource can execute the workflow's tasks.
In addition, when executing a workflow on HPC resources, it can be assumed that all the resources will be available during the execution of the workflows.
These two assumptions are not necessarily true when a computational campaign is executed.
As campaign's workflows are heterogeneous, not all resources will satisfy the space requirements for all the workflows.
Introducing resource dynamicity requires HEFT to take into account resource availability to decide on which resources it will schedule workflows.
In addition, it should update the schedule as resources become unavailable.

% ----------------------------------------------------------------------------
% Campaign Manager definition, requirements, features and capabilities
We propose to design a campaign manager (CM) which, given a campaign, an objective, and a set of constraints, can derive an execution plan by utilizing the proposed makespan HEFT method.
Execution planning for workflows are provided by Pegasus~\cite{deelman2015pegasus}, and ASKALON~\cite{fahringer2005askalon}.
We plan to extend these capability to campaigns.
Figure~\ref{fig:refarch} shows a reference architecture where the CM has two sub-components:
\begin{inparaenum}[(1)]
\item Makespan Calculator which implements HEFT, and
\item an Executor which executes the plan. 
\end{inparaenum}
Workflow execution will be done through an existing workflow management framework (WMF) on HPC resources.
If necessary, CM will adapt the execution plan by updating the workflows to resource mapping decisions. 
These updates will be based on workflows execution metrics provided by the selected WMF such as tasks execution time, overheads calculation and time to completion.
These metrics will be aggregated across workflows resulting in campaign-wide execution metrics.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\textwidth]{figures/CEM_RefArch.pdf}
    \caption{Reference Architecture of a Campaign Manager. Basic 
    subcomponents of Campaign Manager (CM): 1) Makespan Calculation, and 2) Executor. 
    CM communicates decisions to RADICAL-EnTK. CM communicates with HPCs to 
    execute parts of the campaign.}\label{fig:refarch}
\end{figure*}

The Makespan Calculator will be responsible for calculating and optimizing the makespan of a campaign based on a set of resources, and the objective.
The calculator will utilize the extend HEFT algorithm to derive an execution plan of the campaign.
In addition, the Campaign Manager should be able to verify whether a resource is available and update the plan accordingly.

The Executor sub-component is responsible to execute, and monitor the plan by interfacing with a WMF.
Based on the plan the Makespan calculator decided, the Executor submits workflows to a WMF to execute on the selected resource.
This requires the Executor to work upon multiple workflows concurrently.
In addition, it should monitor workflow execution and resource availability.
An important requirement for the executor is to identify the reason of a failing workflow.
When the failure is because the resource is not available the specific workflow may need to be rescheduled and the plan to be updated.

RADICAL-Ensemble Toolkit~\cite{balasubramanian2018harnessing} (EnTK) is a workflow management framework.
We selected to utilize EnTK because it fits the requirements of the target use cases.
EnTK defines workflows as a set of pipelines, each pipeline is a sequence of stages, and in turn each stage a set of tasks.
EnTK support the execution of a sequence of workflows may either reuse resources or request new ones, based on how the user has programmed the application.
EnTK workflow execution is stateful, provides execution metrics, such as task execution time, and supports workflow execution on multiple HPC resources.
Furthermore, it support a pilot runtime system, RADICAL-Pilot~\cite{merzky2019using}, to execute workflows on HPC resources.
Pilot systems submit job placeholders on resources, and are able to execute tasks on the acquired resources.
In addition, the specifics of each individual resource is abstracted and hidden from the upper layer.
As a result, the campaign manager will see a set of resources where workflows should be executed upon.
The proposed campaign manager will interface with EnTK to execute workflows based on derived execution plan.


Initially, we will assume a static campaign with static heterogeneous workflows, executing on static resources.
This will allow us to understand the selected algorithm, and find its performance compared to a random plan.
Next, we will relax the assumption of static resources and work with dynamic resources.
We will introduce resource dynamicity as resource availability, where resources will not be available and the plan would have to be adjusted.
