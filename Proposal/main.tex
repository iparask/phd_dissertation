\input{preamble.tex}

\title{Autonomic Middleware for executing scientific workflows}
\author{Ioannis Paraskevakos \\	Electrical and Computer Engineering \\Rutgers, The State University of New Jersey}

\begin{document}
\maketitle

\abstract{This is where the abstract goes}


\section{Introduction}
Many scientific applications, which use High Performance Computing (HPC) resources, use complex workflows to execute experiments, produce data or acquire them through sensors. These workflows usually involve ensemble of simulation, which are then analyzed. Molecular Dynamics simulations use cases are producing O(100) GBs~\cite{cheatham2015impact} of data that would benefit from an online analysis to drive the simulations. Satellite imagery use cases acquire high resolution satellite imagery that needs to be analyzed in a bounded amount of time. Use cases with sensors acquire data in production lines and require fast data analysis to maintain quality of service. Based on the volume of data, as well as the goals of the application, the computational resources and quality of service requirements vary significantly. 

\mtnote{Relationship among: use cases, HPC, parallelism, data partitioning, load balancing, data homogeneity/heterogeneity. These are at least two paragraphs where you set the problem space. The following paragraph looks at available solutions, pointing out (interesting?) cases where a specific solution is suboptimal.}

Simulations mainly facilitate MPI to parallelize their execution and reduce the necessary time to produce the data. The parallel algorithms used are executing the same simulation with different initial parameters, making the parallelization relatively straight forward in terms of algorithms. Data analysis requires specific data partitioning, load balancing, different methods and levels of parallelization, and heterogeneous resources --- CPUs, and GPUs. Furthermore, resource acquisition and configuration on HPC is not straightforward. Depending on the requirement of the workflow different type of resources, in matters of type and size may be required. This creates a complex decision space which the user has to tackle so that she is able to finish her experiment.

Data analytics frameworks, such as Spark~\cite{zaharia2010spark} or Dask~\cite{rocklin2015dask}, offer automatic as well as user-defined data partitioning to enable compute parallelism. Both frameworks offer automatic data partitioning. Although, automatic partitioning offers load balancing when the data set is homogeneous, such as a time series, it does not guarantee load balanced execution for datasets where each datum is of different size. These can be datasets of MD trajectories produced by similar simulations, sets of satellite or airborne images, or data acquired by different type of sensors. As a result, time to completion automatic data partitioning may not offer the best approach to minimize time to completion. Furthermore, executing the analysis offline or separately from simulations to tune for every possible dataset and algorithm requires compute time that may be valuable to the solution of a scientific problem.

As scientific applications grow in size, their workflows are becoming an ensemble of simulations with hundreds or thousands of members~\cite{malawski2015algorithms,rietmann2012forward}. Based on the scientific experiment, some ensemble members should continue executing, others finalize all together or restart with different initial conditions. Furthermore, the computational lifespan of large scientific applications can be from several weeks to months and may span a number of resources, creating scientific campaigns. The computational lifespan of such campaigns require that the user has knowledge about the resources able to execute the experiment, the state of the experiment at any given point in time, how to manage data produced during execution, and more. As a result many human-hours are invested in managing scientific campaigns.

In addition to applications, computational resources increase in size and heterogeneity. High Performance Computing resources start offering compute nodes that include from tens to hundreds of CPU cores, local filesystem, several GPUs, arithmetic accelerator, field programmable gate arrays, different levels of memory hierarchy with burst buffers. Summit, for example, at Oak Ridge National Lab, offers 4600 compute nodes with more that 100 CPU cores per node and 6 GPUs. Utilizing multiple heterogeneous resources, to execute parts of the campaign, becomes a complex task. As a result, users use the resource they are more comfortable with, although they may have access to multiple resources.

\mtnote{Explain: the reason behind these assumptions; why they are consistent with these reasons; why the resulting class of problems is (still) interesting.}

Workflow execution systems so far allow the execution and monitoring of scientific workflow applications requiring from the user to manage the steps of a scientific campaign. They do not seem to enable capabilities that will allow users to define and execute a campaign or large scientific applications. The user remains responsible to manage members of a campaign by monitoring their state, adjusting based on results that were obtained. As a result, the problem of providing a middleware software layer that allows users to define scientific campaigns on High Performance Computing resources remains open. Thus, the question becomes how can a middleware software layer provide capabilities to users to define a campaign and application requirements, such computational requirements, life span, decision making heuristics, such that the campaign is executed with minimal monitoring and intervention from the user?

This work will be executed under the following assumptions. First, the supported workflows have a simulation/data acquisition phase and an analysis phase. The simulation/data acquisition phase are producing data. The analysis phase is analyzing these data to make a scientific derivation which can be used to start a new phase of simulation/data acquisition. These two phases are executed interchangeably to achieve a well defined scientific goal. Second, there is no a priori knowledge about the data volume that is produced. Third, the workflows are executing in High Performance Computing resources, such as those offered by XSEDE. Fourth, there is a constant requirement is to reduce the time to execution of the analysis such that the time used by simulations is maximized.

\mtnote{Elaborate the problem just introduced creating a link to autonomic computing. The flow/story is: given this problem, given the properties of this problem (paragraph), autonomic computing can offer a solution (another paragraph).}

Autonomic computing software provides properties of self-configuration, self-optimization, and self-regulation. Self-configuration refers to the ability of the system to automatically configure its own execution environment and process. Self-optimization refers to the ability of the system to function near optimally by monitoring and controlling its resources. Self-regulation refers to the ability of the system to maintain a defined goal without any external input from a user. Thus, a middleware that offers these properties to support data analytics can provide a solution for this problem.

In this proposal, we present research done to achieve scalable analysis solutions on HPC resources and to understand the performance requirements and behaviors. We propose a middleware for autonomic data-intensive analytics on HPC that captures the requirements and assumptions mentioned. In addition, we discuss the challenges of this work and provide a timeline of execution.

\section{Current Research}

\subsection{Related Work}

The SelfLet framework~\cite{bindelli2008building} is an autonomic software system. This system provides a set of autonomic components, called SelfLets, that operate in order to achieve a goal. Each SelfLet provides a set of services, behaviors and policies. A SelfLet can be part of a network which allows it to utilize services from other SelfLets to achieve its goal. A SelfLet system has been used in a distributed sense to achieve load balanced service requests~\cite{calcavecchia2010emergence}.

The DIOS++ framework~\cite{liu2003dios} offers a rule-based autonomic management system for scientific applications. DIOS++ provides abstractions to create sensor and actuator which allow runtime monitoring and control, a distributed network to connect and manage the sensor/actuators and a distributed engine to execute parts of the application based on user defined rules.

A general architecture, as well as a prototypical implementation, is provided by Jha et al. in~\cite{jha2009self}. This system consists of a resource manager, an autonomic tuner and an application. It defines an Application objective, similarly to SelfLets goals, which is then translated to a set of measurable requirements, using well defined metrics, that the application should meet. These objectives are achieved through user defined mechanisms. Based on the objective and a set of mechanisms, the system can then define necessary strategies.

Cloud4IoT~\cite{pizzolli2016cloud4iot} is an autonomic system which allows automatic deployment and configuration of data-intensive workloads on cloud and edge devices, and Internet-of-Things (IoT) application support. Cloud4IoT proposes an architecture where a Cloud is used as a central entity of computation, Edge devices are used to execute the initial processing steps of a data-intensive application, and IoT gateways are used as the gateways for sensors to connect to the system and transmit data.

CometCloud~\cite{diazmontes2015cometcloud} is an autonomic software system that enables scientific applications on a federation of resources. It has been used to enable simulation and data-intensive applications on heterogeneous resources, HPC and Cloud, that consumed million of core hours.

\subsection{Conceptual Model for Task-Parallel framework selections}
Tasks-parallel applications involve partitioning a workload into a set of self-contained units of work. Based on the application, these tasks can be independent or coupled with varying degrees of data dependencies. Scientific workflows exploit task parallelism for simulations as well as data analysis.

In~\cite{paraskevakos2018task}, we investigated the suitability of task-parallel frameworks for Molecular Dynamics trajectory data analysis. The analysis included Spark~\cite{zaharia2010spark}, Dask~\cite{rocklin2015dask}, and RADICAL-Pilot~\cite{merzky2019using}. 

Spark~\cite{zaharia2010spark} and Dask~\cite{rocklin2015dask} are two Big Data frameworks. Both provide MapReduce abstractions and are optimized for parallel processing of large data volumes, interactive analytics and machine learning. Their runtime engines can automatically partition data, generate parallel tasks, and execute them on a cluster. In addition, Spark offers in-memory capabilities allowing caching data that are read multiple times, making it suited for interactive analytics and iterative machine learning algorithms. Dask also provides a MapReduce API (Dask Bags). Furthermore, Dask’s API is more versatile, allowing custom workflows and parallel vector/matrix computations.

Spark~\cite{zaharia2010spark} and Dask~\cite{rocklin2015dask} provide execution parallelization based on members of a dataset. These frameworks provide map-reduce abstractions and are optimized for parallelizing the execution for large datasets. Their runtime engines can automatically partition data, generate parallel tasks, and execute them on a cluster. In addition, Spark offers in-memory capabilities allowing caching data that are read multiple times, making it suited for interactive analytics and iterative machine learning algorithms. Dask also provides a MapReduce API (Dask Bags). Furthermore, Dask’s API is more versatile, allowing custom workflows and parallel vector/matrix computations. 

\subsection{Data Analysis Design Selection}

Write about publication number 2

\section{Proposed Research}

In our research so far, we discussed the understanding of data analytics for various use cases on HPC systems. In addition, we identified the need for execution of scientific workflows with minimum user intervention independent from domain. In this section, we motivate and propose an autonomic middleware for configuring, monitor and adapting the execution of scientific experiments on HPCs.

\subsection{Proposed Topic}
Monitoring, regulating, and configuring large scale scientific workflows require dedicated human resources that may not be available at any given point in time. Autonomic systems offer properties of self-monitoring, self-configuring, and self-regulating. As a result, they can be used to support scientific workflow execution.

We propose creating an autonomic middleware that will be able to offer these self-* properties to scientific workflows. This middleware will be responsible in monitoring, configuring and regulating the execution based on policies and rules defined by the users. 

Self-monitoring, in this use case, defines the ability of the middleware to evaluate the current state of the workflow. A state full workflow execution engine, such as EnTK~\cite{balasubramanian2018harnessing}, Dask~\cite{rocklin2015dask}, is necessary. By sampling the state of the individual components of the workflow, the proposed system can realize the overall state of the execution. Self-configuration can be achieved in multiple levels. One is deciding which resources (CPU, GPU, or memory based) will be used for the execution of the workflow. For example, HPCs offer nodes with different memory sizes, type and count of GPUs, number of nodes per job. These decisions can be made from the proposed middlewale. Another mode of self-configuration is deciding how to execute the workflow when different possible implementations and designs exist.  Self-regulation can be achieved by defining a set of rules and actions that can be executed. The user will be able to define a set of possible outcomes of different states of hers workflow. Based on these outcomes changes in the workflow can be made during runtime. This will allow the system to change what the workflow executes based on the scientific goal.

\subsection{Proposed Timelime}
\begin{table*}
	\centering
	\begin{tabular}{ |p{1.25cm}|p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm} p{1.25cm}|}
		%
		\hline
		Phases  & Month 1 & Month 2 & Month 3 & Month 4 & Month 5 & Month 6 & Month 7 & Month 8 & Month 9 \\\hline\hline
		Phase 1 &         &         &         &         &         &         &         &         &         \\\hline
		Phase 2 &         &         &         &         &         &         &         &         &         \\\hline
		Phase 3 &         &         &         &         &         &         &         &         &         \\\hline
		Phase 4 &         &         &         &         &         &         &         &         &         \\\hline
	\end{tabular}
\caption{Planned time-line of proposed research}\label{tab:work_plan}
\end{table*}

\subsubsection{Phase 1: Design and implementation of prototype}

Month 1 would include design discussions for a prototype of the execution system. Design will be finalized and implementation will be completed by Month 3. Capability to self-configure in terms of resources based on the desired workflow will mark the success of Phase 1.

\subsubsection{Phase 2: Validation of the execution system}

Implement the rest of the \textit{self-*} properties

\subsubsection{Phase 3: Integration of with scientific workflows}


\subsubsection{Phase 4: Investigation for an empirical model}

Experiments with synthetic and real applications with the prototype as well as scientific workflows will be used to investigate and derive empirical models. These models may be used to further generalize the behavior of the autonomic middleware based on application requirements. Phase 4 will overlap with Phase 2 and 3 to utilize experiments done during those phases.

% ---------------------------------------------------------------------------
% Why
\subsection{Significance and impact of work}


% ---------------------------------------------------------------------------
% Challenges
\subsection{Challenges/Risks}

We estimate the proposed work, divided into four major phases, to take 9 months and we allocate 3 months to account for unforeseen circumstances. We would like to keep the committee aware of the following challenges that we see:

\begin{itemize}
	\item Design and Implementation (phase 2) is iterative and special attention needs to be given to the number of iterations against specific objectives, given the time-line.
    \item All experiments performed on HPC systems are subject to variable queue times and may limit the number of experiments performed in phase 2 and 4.
	\item Although the middleware will be well tested (80--90\% of the code base will be covered by unit tests) and less susceptible to major changes, RADICAL-Pilot is known to be less stable and is susceptible to changes as it serves multiple projects. Stability of RADICAL-Pilot is considered in the estimates, but needs to be made aware to the committee.
	\item Lastly, the HPC systems themselves may often become inaccessible due to unplanned outages.
\end{itemize}

\bibliographystyle{plain}
\bibliography{proposal}
\end{document}