\label{ch:conclusions}

This dissertation presents research in advancing the state-of-the-art in executing computational campaigns with data-intensive workflows on HPC resources.
This enables scientists to remove themselves from monitoring and executing a campaign and focus more on scientific innovations.

%% Chapter 2
As data-intensive applications are not well support on HPC, we motivated the use of the Pilot-Abstraction as an integrating concept between HPC and data analytics.
We demonstrated that the Pilot-Abstraction strengthens the state of practice in utilizing HPC resources in conjunction with emerging data analytics frameworks by allowing users to combine a diverse set of tools running on heterogeneous infrastructure consisting of HPC.
%% Chapter 3
Further, we investigated the use of these different programming abstractions and frameworks for implementing a range of algorithms for MD trajectory analysis. 
Based on our analysis, we provide a conceptual framework that enables application developers to evaluate task parallel frameworks with respect to application requirements. 
Our benchmarks enable them to quantitatively assess framework performance as well as the performance of different implementations. 

%% Chapter 4
There are though some applications that show both data and compute-intensive characteristics and are not necessarily well supported by data-parallel frameworks.
We investigated three architecturally equivalent designs to execute a data-driven compute intensive workflow.
We saw that such workflows from early binding of data to compute nodes so to maximize data and compute affinity and equally balance input across nodes.
This approach minimizes the overall time to completion of this type of workflows while maximizing resource utilization compared to late binding tasks to compute nodes.
In presence of large amount of data, late binding implies copying, replicating or accessing data over network and at runtime.
We showed that, in contemporary HPC infrastructures, this is too costly both for resource utilization and total time to completion.

%% Chapter 5
Tools that support computational campaigns are making assumptions about the resources and the middleware they are utilizing, are monolithic software systems, and tend to be domain specific
In response to these limitations, we designed and implemented a campaign manager prototype which is domain agnostic and adheres to the building blocks approach.
The campaign manager creates an execution plan and executes the campaign on HPC resources.
Further, by utilizing a discrete time simulator we are able to simulate the execution of campaigns with different sizes and types of workflows in minimal time.

%% Chapter 6
There is a plethora of algorithms that can create an execution plan for a computational campaign.
As a result, we investigated three planning algorithms, HEFT, a genetic algorithm (GA) and L2FF, and experimentally compared their plans performance in terms of makespan.
Based on our analysis, we derive the algorithmic characteristics that affect the plan performance and its sensitivity to resource dynamism and workflow runtime estimation uncertainty.
Finally, we provide a conceptual model where users can select a planning algorithm based on their campaign characteristics, the resources they have access and their computational objective.


\section{Future Work}
The work of this dissertation is invariant of the scientific domain and HPC resources that a campaign can utilize.
Several strands of future work open to efficiently and effectively support any type of computational campaign on HPC resources.

First, the campaign manager and specifically its planner, assumes that all workflows are able to execute an all available resources.
In reality, workflows may have different resource requirements, number and type of resources, and cannot execute on all resources.
We plan to extend the set of algorithms the planner uses to algorithms that define resource requirements for workflows and evaluate their performance.
In addition, all planning algorithms we currently used early bind workflows on resources.
On the other hand, late binding algorithms are responding to events during runtime and their execution plan evolves as the campaign executes.
Late binding algorithms can offer the same performance as early bind algorithm on homogeneous resources and we believe that they can offer similar performance with HEFT on heterogeneous resources.
As a result, we plan to investigate the performance of a late binding algorithm and compare it with HEFT's performance.

Second, we will extend RADICAL-EnTK and RADICAL-Pilot to support data-driven compute intensive workflows on HPC resources.
This will allow us to co-locate data and compute and increase the utilization of HPC resources for this type of workflows.
We will use our overheads characterization as a baseline to evaluate our production-grade implementation and further improve the efficiency of our middleware. 
Further, we will apply the presented experimental methodology to other use cases and infrastructures, measuring the trade offs imposed by other types of task heterogeneity, including multi-core or multi-GPU tasks that extends beyond a single compute node.

Finally, the campaign manager prototype does not support yet the actual execution of campaigns.
As we have an understanding of the requirements to support the execution of a campaign, we want to investigate what are the requirements to utilize an actual workflow management framework (WMF).
RADICAL-EnTK will be our first choice, as it fits the requirements of the target use cases, and utilizes a pilot framework as runtime system.

