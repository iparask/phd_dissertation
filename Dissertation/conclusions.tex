% !TEX root = main.tex

This dissertation investigated the execution of computational campaigns with
data- and compute-intensive workflows on HPC resources. Our contributions to the
field offer to domain scientists a better understanding of the capabilities
needed to efficiently and effectively execute computational campaigns on HPC
resources. Further, we provide the computational tools, methodologies and
experimental understanding needed to design a software manager to plan and
execute computational campaigns on high performance computing (HPC)
platforms.

\section{Contributions}

%% Chapter 2
Computational campaigns on HPC resources can execute compute- or data- intensive
workflows. Compute-intensive workflows are at the epicenter of high-performance
scientific computing and account for the production of an immense amount of
data. Compute intensive workflows execute either a single, very large and
long-running executable or an ensemble of smaller compute-intensive
tasks~\cite{balasubramanian2018harnessing}. As the need for analyzing data on
HPC resources increases, the efficient and effective execution of data-intensive
workflows on HPC becomes important. Contrary to compute-intensive workflows,
data-intensive workflows execute a large number of short-running tasks in
multiple stages which can be I/O, memory and compute bound.

As data-intensive workflows and applications are not well supported on HPC
infrastructures, we showed how the Pilot-Abstraction can be used as an
integrating concept between HPC resources and data analytics workflows in
chapter~\ref{ch:pilot-data-hadoop}. We developed RP-YARN, extending
RADICAL-Pilot, an implementation of the Pilot-Abstraction, to support Hadoop and
Spark on HPC resources (see \S~\ref{sec:integration_mode}). Our analysis in
\S~\ref{ssec:kmeans} showed that RP-YARN reduces the time to completion of a
data-intensive workflow. We concluded that the Pilot-Abstraction can increase
HPC resources utilization when executing data analytics workflows with data 
analytics frameworks.

%% Chapter 3
Data analytics frameworks provide different programming abstractions and
resource capabilities. Selecting a suitable framework becomes crucial to
increase resource utilization and reduce development effort.
In chapter~\ref{ch:task-par}, we investigated the use of three task-parallel
frameworks---Spark, Dask and RADICAL-Pilot---to implement a range of algorithms
for MD trajectory analysis, typically used in data analysis workflows. We found
that, for embarrassingly parallel applications with coarse  grained-tasks, the
framework selection does not affect the performance. As shown
in~\S~\ref{sec:psa}, all three frameworks showed similar performance and
speedups. For fine-grained data-parallel applications, a  data-parallel
framework is more suitable. \S~\ref{sec:leaflet} shows our investigation of a
MapReduce style algorithm and how Spark and Dask showed better performance than
RADICAL-Pilot, with Spark offering linear speedups.

In addition to performance gains, developers should take into account usability
and programmability aspects when selecting a task parallel framework. Spark and
Dask, as shown in~\S~\ref{sec:impl_exp}, required to tune the number of tasks to
achieve the desired performance (programmability aspect). The programming
language of the selected framework affects its usability as it may introduce
overheads due to transferring data between different languages space, e.g.,
between Python and Java as required by PySpark. In~\S~\ref{sec:task_sel_model},
we provide a conceptual framework that enables application developers to
evaluate and select task parallel frameworks with respect to application
requirements. The conceptual model, in conjunction with the Pilot-Abstraction,
provide a methodology for application developers to maximize resource utilization
while reducing the engineering effort needed to develop and execute data analysis
workflows on HPC resources.

%% Chapter 4
Some data-analysis workflows and applications are both data- and
compute-intensive and, as such, are not well supported by frameworks
specifically designed for data parallelism. In Chapter~\ref{ch:designs}, we
investigated three functionally equivalent designs to execute a data- and
compute-intensive workflow. We characterized the designs by measuring task
execution time (\S\ref{ssec:des1analysis} and \S\ref{ssec:des2analysis}),
resource utilization (\S\ref{ssec:exp2}), workflow time to completion and design
overheads (\S~\ref{ssec:exp3}). Based on our analysis, we found that late
binding data to compute nodes, especially in the presence of large amount of
data, implies copying, replicating or accessing data over network and at
runtime. We showed that, in HPC infrastructures, this is too costly both for
resource utilization and total time to completion. Further, we found that early
binding of data to compute and equally balancing input across nodes reduced time
to completion and increased resource utilization. This result provides
information on the design properties that workflow management frameworks
should have to efficiently and effectively execute data-driven and
compute-intensive workflows. As a consequence, application developers should
develop their workflows with a framework that allows to early bind and balance
data to compute nodes.

%% Chapter 5
Having established our understanding on how to effectively and efficiently
support the execution of data- and compute-intensive workflows on HPC, we
discussed the design of a campaign manager in Chapter~\ref{ch:cmanager}. We
discussed three actual computational campaigns, eliciting the requirements for a
campaign manager. Based on those requirements, we designed and implemented a
campaign manager prototype which is domain-agnostic and adheres to the building
blocks design approach. We utilized a discrete event simulator
(\S\ref{sec:cm_impl}) to simulate the execution of a computational campaign on
HPC resources. We validated the correctness of our design by executing campaigns
of different sizes on different number of resources (\S\ref{sec:cm_impl}).

%% Chapter 6
As the campaign manager derives an execution plan for a computational campaign
and given resources, in Chapter~\ref{ch:campaigns} we investigated three
planning algorithms---HEFT, a genetic algorithm (GA) and L2FF---to plan the
execution of a campaign. We experimentally compared their plan performance in
terms of makespan and how sensitive the plans are to resource dynamism and
workflow runtime uncertainty (\S\ref{sec:algo_perf_comp}). As computational
campaigns utilize heterogeneous resources, in \S\ref{ssec:makespan_comp} we
showed that HEFT provides at least two times, and up to an order of magnitude
better makespan than GA and L2FF. Further, we found that plans using HEFT are on average 
more sensitive to resource dynamism than plans using GA and L2FF
(\S\ref{ssec:res_dyn}) and workflow runtime uncertainty
(\S\ref{ssec:work_uncert}). Based on our experimental analysis, we discussed
the algorithmic characteristics that affect the plan performance and
sensitivity to resource dynamism and workflow runtime estimation uncertainty.
Finally, we provided a conceptual model that users can use to select a
planning algorithm based on their campaign characteristics, the resources they
target and their computational objective.

This work has immediate impact on use cases from earth science domains that
analyze very high resolution satellite imagery. Scientists from those domains
want to execute computational campaigns with multiple workflows that analyze
imagery from different calendar years and multiple satellite and terrestrial
sources. New imagery becomes available at a constant low rate stream that, in
turn, require constant analysis. Utilizing the work done to develop a campaign
manager, scientists will be able to continuously and effectively executing
workflows that analyze imagery as it becomes available.

\section{Future Work}

The work of this dissertation is invariant of the scientific domain and HPC
resources that a campaign can utilize. As we move forward to support actual
computational campaigns on HPC resources, we plan to pursue three lines of
research and software development.

First, our campaign manager prototype and specifically its planner, assumes that
all workflows are able to execute on all available resources. In reality,
workflows may have different resource requirements, number and type of
resources, and cannot execute on all resources. We plan to extend the set of
algorithms of the planner with algorithms that define resource requirements for
workflows, and evaluate their makespan performance and sensitivity to resource
dynamism.

Second, the planning algorithms we considered are early binding, i.e., they
early bind workflows to resources. Late binding algorithms respond to events
during runtime and their execution plan evolves as the campaign executes. Late
binding algorithms can offer the same performance as early binding algorithms on
homogeneous resources and we believe that they can offer similar performance to
HEFT on heterogeneous resources. As a result, we plan to characterize the
performance of diverse late binding algorithms and compare them to HEFT's
performance.

Finally, our campaign manager prototype does not support the actual execution of
campaigns but only their simulation. As we have an understanding of the
requirements to support the execution of a campaign, we want to investigate 
the requirements to utilize an actual workflow management framework.
RADICAL-EnTK will be our first choice, as it fits the requirements of the target
use cases, and utilizes a pilot framework as runtime system.
