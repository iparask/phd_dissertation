% !TEX root = main.tex
\label{ch:conclusions}

This dissertation investigated the execution of computational campaigns with
data- and compute-intensive workflows on HPC resources. Our contributions to the
field offer to domain scientists a better understanding of the capabilities
needed to efficiently and effectively execute computational campaigns on HPC
resources. Further, we provide the computational tools, methodologies and
experimental understanding needed to design a software manager to plan and
execute computational campaigns on high performance computing (HPC) platforms.
% This enables scientists to remove themselves from monitoring and executing a
% campaign and focus more on scientific innovations. This research make the
% following contributions.

\section{Contribution}

%% Chapter 2
Data-intensive workflows have different characteristics from compute-based
workflows which are at the epicenter of scientific computing that utilizes HPC
resources\mtnote{That sentence is compressed: you want to express at least three
concepts with it: data- vs compute-intensive workflows; growing role of the
former on HPC; different requirements of data-intensive workflows on HPC.}. As
data-intensive workflows and applications are not well supported on HPC, we
motivated the use of the Pilot-Abstraction as an integrating concept between HPC
and data analytics. As a result, we extended RADICAL-Pilot, an implementation of
the Pilot-Abstraction, to support Hadoop and Spark on HPC resources. Through our
analysis, we showed that the Pilot-Abstraction increases HPC resources
utilization in conjunction with emerging data analytics frameworks.
\mtnote{Please reference explicitly the chapter/section where you made the
contributions you are listing here.}

%% Chapter 3
Data analytics frameworks offer different abstractions and capabilities and
selecting a suitable framework is necessary to increase resource
utilization.\mtnote{Only resource utilization? Is it more general? Something
like: "to minimize development effort, adapt the programming model to the
application requirement and maximize resource utilization"?} We investigated the
use of three task-parallel frameworks---Spark, Dask and RADICAL-Pilot---to
implement a range of algorithms for MD trajectory analysis. We found that, for
embarrassingly parallel applications with coarse grained-tasks, the framework
selection does not affect the performance. For fine-grained data-parallel
applications, a data-parallel framework is more suitable, with Spark offering
linear speedups. Based on our analysis, we provide a conceptual framework that
enables application developers to evaluate task parallel frameworks with respect
to application requirements.\mtnote{Add references as suggested above but, more
in general, I would like to see more structure. You have two types of
contriubtions: performance (quantitative) and a mix of usability/programmability
(qualitative). I do not remember well, but reading the chapter those
contribution were clearly separated. I would make this distinction explicit here
too and organize the paragraph around it.}

%% Chapter 4
Some data-analysis workflows and applications are both data- and
compute-intensive and, as such, are not well supported by data-parallel
frameworks. We investigated three architecturally equivalent designs to execute
a data- and compute-intensive workflow. We saw that such workflows benefit from
early binding of data to compute nodes, maximizing data and compute affinity,
and equally balancing input across nodes. This approach minimizes the overall
time to completion of this type of workflows, while maximizing resource
utilization compared to late binding tasks to compute nodes. In presence of
large amount of data, late binding implies copying, replicating or accessing
data over network and at runtime. We showed that, in contemporary HPC
infrastructures, this is too costly both for resource utilization and total time
to completion.

%% Chapter 5
Having established our understanding on how to effectively and efficiently
execute data- and compute-intensive workflows on HPC, we discussed three
scientific computational campaigns. Based on their requirements, we designed and
implemented a campaign manager prototype which is domain agnostic and adheres to
the building blocks approach. The campaign manager prototype creates an
execution plan and executes the campaign on HPC resources. We validated the
design by simulating the execution of computational campaigns.

Finally, we investigated three planning algorithms---HEFT, a genetic algorithm
(GA) and L2FF---to plan the execution of a campaign and experimentally compared
their plan performance in terms of makespan. As computational campaigns utilize
heterogeneous resources, we observed that HEFT provided better makespan than GA
and L2FF. In addition, we measured how sensitive plans that use HEFT, GA and
L2FF are to resource performance changes and to uncertain information about
workflow runtime. Based on our experimental analysis, we derive the algorithmic
characteristics that affect the plan performance and its sensitivity to resource
dynamism and workflow runtime estimation uncertainty. We conclude by providing a
conceptual model where users can select a planning algorithm based on their
campaign characteristics, the resources they have access and their computational
objective.

\section{Future Work}

The work of this dissertation is invariant of the scientific domain and HPC
resources that a campaign can utilize. As we move forward to support actual
computational campaigns on HPC resources, we see three strands of research and
software development that can be pursued.

First, the campaign manager and specifically its planner, assumes that all
workflows are able to execute on all available resources. In reality,
workflows may have different resource requirements, number and type of
resources, and cannot execute on all resources. We plan to extend the set of
algorithms the planner uses with algorithms that define resource requirements
for workflows and evaluate their makespan performance and sensitivity to
resource dynamism.

Second, all planning algorithms we considered early bind workflows on resources.
Late binding algorithms respond to events during runtime and their execution
plan evolves as the campaign executes. Late binding algorithms can offer the
same performance as early binding algorithms on homogeneous resources and we
believe that they can offer similar performance with HEFT on heterogeneous
resources. As a result, we plan to characterize the performance of a late
binding algorithms and compare it with HEFT's performance.

Finally, the campaign manager prototype does not support the actual execution of
campaigns. As we have an understanding of the requirements to support the
execution of a campaign, we want to investigate what are the requirements to
utilize an actual workflow management framework. RADICAL-EnTK will be our
first choice, as it fits the requirements of the target use cases, and utilizes
a pilot framework as runtime system.
