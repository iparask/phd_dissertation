% !TEX root = main.tex
\label{ch:campaigns}
Computational campaigns enact an execution to achieve a computation objective under given requirements and constraints.
The computational objective is a set of values selected by the user for a set of metrics.
Some of these metrics are time to completion and throughput.
Requirements describe the minimum amount and type of resources needed to execute each workflow of the campaign, while the constrains are the conditions that bound the the execution, such as resource availability, resource capacity or costs.

The objective of a campaign can be translated to a computational objective function that would either minimize or maximize a metric.
Among the many metrics that could be considered, the most common one is the total time taken by a campaign to execute, also known as makespan.
An execution plan of a campaign is a mapping between workflows and resource to execute upon.
Calculating the makespan of a campaign means finding an execution plan that satisfies the computational objective function.

A computational campaign can utilize several resources concurrently.
These resources can either be homogeneous or heterogeneous as they offer different type of computational resources.
One aspect of this heterogeneity is the performance each resources has in terms of number of operations per second.
High Performance Computing (HPC) resources allow the concurrent execution of workflows on homogeneous resources by utilizing different sets of the same type of compute nodes for each workflow.
In addition, resources can be heterogeneous by accessing completely different HPC resources, but also by utilizing different types of compute nodes on the resource.

Uncertainty about the execution of a campaign can rise for several reasons, such resource dynamism or workflow runtime uncertainty.
HPC resources are governed via policies that affect their performance, such as power regulating policies.
In addition, HPC offer shared filesystems and networks which are used by multiple users concurrently.
As a consequence, their performance is dynamic and change over time, which in turn creates uncertainty on the amount of time a workflow will execute for.
Further, users do not necessarily know the exact runtime for every workflow in a campaign, but an estimation.
As a result, the runtime of a workflow is not know before the execution.
Both resource dynamism and workflow runtime uncertainty have the potential to affect the makespan of the campaign.

There is a plethora of algorithms which calculate execution plan to minimize the makespan of a workflow offline~\cite{lu2019review} and before the workflow execution.
These type of algorithms are important to planning the execution of a campaign as they provide information about the campaign's expected makespan.
As a result, users will know whether there is a plan that satisfies their computational objective.
Selecting an algorithm that is suitable for executing a given campaign on a set of resources is not necessarily trivial.
To the best of our knowledge, there is not a methodology that allows to compare planning algorithms and select the most suitable for planning the execution of the campaign.
Thus, we select three algorithms that each represents a larger family and compare their performance.
Based on the comparison, we make a case of which algorithm to select given a computational campaign, it requirements and constraints.

The chapter offers the following contributions:
\begin{inparaenum}[i)]
    \item an experiment-based methodology to compare planning algorithms performance that does not depend on the considered use case and computing framework;
    \item a conceptual framework for selecting planning algorithms based on the algorithm, campaign and resources characteristics
\end{inparaenum}

The chapter is organized as follows: \S~\ref{sec:makespan_calc} discusses the calculation of the makespan of a campaign given the set of assumptions of this work.
Section \S~\ref{sec:algo} discusses three selected makespan calculation algorithms and \S~\ref{sec:algo_perf_comp} compares their performance, i.e the derived makespan.
Finally, section \S~\ref{sec:cf_algo_sel} presents a conceptual framework that will allow users to best select a planning algorithm.

% -------------------------------------------------------------------
\section{Calculating the Makespan of a Campaign}
\label{sec:makespan_calc}
The way workflows of a given campaign are mapped to resources can affect the makespan calculation. 
Figure~\ref{fig:example_makespan} shows an example of a campaign with workflows of different size and execution times, and the makespans that two different mappings produce.
The makespan of the campaign on the left sub~-figure is $20$, while on the right it is $16$.
In addition, the size of the workflows, i.e., the number of resources they require, becomes relevant and resources may be underutilized, as shown in Figure~\ref{fig:example_makespan}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=.99\textwidth]{figures/campaign/plan_comp.pdf}
    \caption{Comparison of different campaign execution plans. Based on workflow mapping on resources makespan and resource utilization is different.}\label{fig:example_makespan}
\end{figure*}

We are making a set of assumptions which we do not relax during the analysis of a model that calculates the makespan of a campaign.
These are:
\begin{inparaenum}[(1)]
    \item a workflow is an atomic unit and cannot be decomposed;
    \item workflow resource request is sufficient to execute the workflow;
    \item a resource is an aggregate of computing capabilities;
    \item every workflow of a given campaign can be executed on the given resources;
    \item random resource selection is based on a uniform distribution;
    \item only one workflow can be executed on a resource at any point in time; and
    \item workflows can be homogeneous or heterogeneous in space---maximum number of resources they need, and time---the amount of time they are executing.
\end{inparaenum}

We denote a computational campaign as $C = [w_{i}: 1 \leq i \leq N_{C}]$, where $w_{i}$ is a workflow and $N_{C}$ is the total number of workflows, $R = [ r_{j}: 1 \leq j \leq N_{R}]$ is a set of available resources, where $r_{j}$ is a resource and $N_{R}$ is the total number of available resources, and $ M(C,R) = [(w_i, r_j): 1 \leq i \leq N_{C}, r_j \in R] $ is a mapping function of workflows onto resources.
In addition, we denote the execution time of a workflow as $Tx_{w_{i}}$, the makespan of campaign $C$ as $TTX_{C}$, and the makespan of campaign $C$ for a given mapping function $ M $ as $TTX_{C}(M)$.
Assumption~\#3 allows to abstract the resource implementation details, as workflows can be executed on different resources such as HPCs, Clouds, pilots and more. 
%Lastly, we will assume initially homogeneous resources as it simplifies the formalization of the problem.

With a single resource, i.e., $N_{R} = 1$, the workflows of a campaign will be executed sequentially, regardless the execution order or if the workflows are homogeneous or heterogeneous.
As a result the makespan of the campaign is:
\begin{equation}
   TTX_{C} = \sum_{i=1}^{N_{C}}Tx_{w_{i}} 
\end{equation}

With multiple resources, i.e., $1 < N_{R} < N_{C}$, the workflows of a campaign can be executed concurrently.
With homogeneous resources, this is semantically equivalent to executing on a single resource large enough to allow concurrent workflow execution, where each workflow executes on a resource partition. 
Because of assumptions~\#4 and~\#7, executing homogeneous or heterogeneous in space workflows has the same makespan.
A random mapping of workflows onto resource will have a makespan:
\begin{equation}
   TTX_{C}(Random) \geq \frac{1}{N_{R}}\sum_{i=1}^{N_{C}} Tx_{w_{i}} 
\end{equation}
Given multiple homogeneous resources, when executing workflows that are heterogeneous in time and that can be homogeneous or heterogeneous in space, the makespan of the campaign for a given mapping function $ M $ is:
\begin{equation}
TTX_{C}(M) = \max_{r_{j}\in R}\Big\{\sum_{w_{i}\in M(C,r_{j})}Tx_{w_{i}}\Big\}
\label{eq:makespan}
\end{equation}
Relaxing the assumption that resources are heterogeneous, in the performance they offer, affects the result of the mapping function $ M $.
As a result, Eq.~\ref{eq:makespan} holds in calculating the makespan of the campaign.

Computational campaigns execute to achieve an objective.
We consider as objective the total time of completion of the campaign.
As a result, the objective function of a plan is translating in minimizing the value of eq~\ref{eq:makespan}.
As objective function is:
\begin{equation}
    \min(\max_{r_{j}\in R}\Big\{\sum_{w_{i}\in M(C,r_{j})}Tx_{w_{i}}\Big\})
\end{equation}
In the next section, we discuss a set of algorithms that try to satisfy this objective function.

% -------------------------------------------------------------------
\section{Planning Algorithms}
\label{sec:algo}

There is a plethora of methods and algorithms to calculate and optimize the makespan of a workflow~\cite{lu2019review}, including queuing networks~\cite{yao2019throughput,bao2019performance}, domain specific languages~\cite{carothers2017durango,maheshwari2016workflow}, and machine learning~\cite{witt2019predictive,pumma2017runtime}.
From this plethora of selections, we selected to investigate three algorithms, each one representative of a larger family of algorithms.
These are Heterogeneous Earlier Finish Time algorithm (HEFT)~\cite{topcuoglu2002performance}, a genetic algorithm~\cite{page2005algorithm} and a simple heuristic algorithm.

The following sections present the selected algorithms in more detail.
In addition to the algorithmic details, we discuss the knowledge and the information each algorithm produces to derive a plan.
A summary of the algorithms characteristics are shown in Table~\ref{tab:sched_algo}.

\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        &\textbf{HEFT}     &\textbf{Genetic Algorithm} &\textbf{L2FF} & \textbf{Random} \\
        \midrule
        Decision Policy   &Deterministic &Convergence Criteria &Deterministic& Deterministic\\
        Initial State    &Blank &Semi-random &Blank & Blank\\
        \midrule
        \multicolumn{5}{l}{\textbf{Initial Information}}\\\midrule
        Workflow Operations &Yes & Yes & Yes & No\\
        Resource Performance &Yes &Yes &Yes & No\\
        \midrule
        Produced Knowledge& Resource availability& Resource availability&None&None\\
        \bottomrule
    \end{tabular}
    \caption{Basic characteristics of selected planning algorithms.\label{tab:sched_algo}}
\end{table}

% -------------------------------------------------------------------
\subsection{Heterogeneous Earlier Finish Time (HEFT) algorithm}
\label{algo:heft}
List scheduling algorithms represent a general family of heuristic based scheduling algorithms to schedule workflows described as direct acyclic graphs~\cite{dong2006scheduling,list_sched_wiki}. 
These algorithms assign priorities to tasks based on a heuristic and order them in an non-increasing order.
Then, this list is traversed and tasks are assigned to resources.
HEFT is a classic example of this type of algorithms~\cite{dong2006scheduling}.

HEFT is a scheduling algorithm which calculates the makespan of a workflow on heterogeneous resources, in terms of performance.
HEFT has been implemented as part of the planning capabilities in Pegasus~\cite{deelman2015pegasus} and ASKALON~\cite{fahringer2005askalon} amongst other algorithms.
HEFT has been shown to provide better performance in terms of makespan minimization compared to other mapping algorithms~\cite{topcuoglu2002performance,fahringer2005askalon,canon2008comparative}. 
Furthermore, there has been some initial research to extend HEFT to resources that provide CPU and GPUs~\cite{shetti2013optimization}, as well as a HEFT extension on dynamic resources~\cite{dong2007pfas}. 

HEFT makes two important assumptions when used on workflows: 
\begin{inparaenum}[(1)] 
    \item any task in a workflow can be executed on all available resources; and 
    \item all resources are initially available.
\end{inparaenum}
HEFT is used to derive an execution plan for workflows, i.e., the execution order and resource placement of tasks that comprise a workflow.
HEFT complexity is proportional to the number of dependencies between tasks and the number of resources offered. 
Since we are interested in campaigns, our HEFT extension provides an execution plan based on workflows as atomic units instead of tasks.
Algorithm~\ref{alg:heft} shows HEFT for placing independent heterogeneous workflows on heterogeneous resources.

\begin{algorithm}[ht]
    \caption{Heterogeneous Earliest Finish Time (HEFT) algorithm}
    \label{alg:heft}
    \begin{algorithmic}[1]
        \Procedure{HEFT}{$W$,$R$}\Comment{$W$ and $R$ are a set of workflow and resources respectively}
        \State \texttt{Calculate the computation cost $w_{tx}^{ij}$ of each workflow for all resources}
        \State \texttt{Assign $rank_i = \overline{w_{i}} = \nicefrac{\sum_{j=1}^{|R|}w_{tx}^{ij}}{|R|}$}
        \State \texttt{Sort workflows in non-increasing order of $rank_i$}
        \While{unscheduled workflows}
        \State \texttt{Select the first workflow $\tilde{w}$ from the sorted list}
        \For{$\forall r_{j}$ in $R$}
        \State\texttt{Compute earliest finish time for $\tilde{w}$ on $r_{j}$, $eft_{\tilde{w},r_j}$ }
        \EndFor
        \State \texttt{Assign  $\tilde{w}$ on $r_k$ with $\min{(eft_{\tilde{w},r_j})}$}
        \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

HEFT has three main algorithmic characteristics:
\begin{inparaenum}[1)]
    \item creates a priority list of workflows,
    \item produces an expectation when resources are available and
    \item uses a deterministic heuristic
\end{inparaenum}
HEFT initially calculates the average execution time of a workflow on all resources and creates a priority list with the longest workflow first.
In addition, HEFT initializes a vector where each element is when the respective resource is available.
Then, it pulls the first workflow from the priority list and calculates when it will finish at each resource.
Finally, it places the workflow on the resource that will finish it earlier and updates the resource availability.
HEFT ends when all workflows are placed on a resource.

   
% -------------------------------------------------------------------
\subsection{Genetic Algorithm}
\label{algo:gen}
Genetic algorithms represent another family of algorithms that are been used to minimize the makespan of workflows~\cite{dong2006scheduling}.
Genetic algorithms start from an initial set of possible solutions, iterate and improve them at every iterations.
In general, genetic algorithms follow the following procedure:
\begin{inparaenum}[(i)]
    \item create an initial population, where the population is a set of possible plans;
    \item evaluate the population members based on a fitness function, which calculates the makespan of the workflow;
    \item reproduce by selecting population members, either randomly or based on their fitness value, and generate a new set of possible plans;
    \item mutate, where randomly selected tasks from a population member are reassigned to resources; and
    \item re-evaluate the members in the population to check convergence.
\end{inparaenum}

The selected genetic algorithm~\cite{page2005algorithm} is developed to support the placement of independent tasks on heterogeneous resources.
It assumes that all tasks can be executed on all available resources, are independent and indivisible.
These assumptions are in accordance with the assumptions made for workflows in a campaign.
As a result, this algorithm can be extended to support scientific campaigns.
The pseudocode of the selected genetic algorithm is shown in Algorithm~\ref{alg:gen_algo}.

\begin{algorithm}[ht]
    \caption{Genetic Algorithm}
    \label{alg:gen_algo}
    \begin{algorithmic}[1]
        \Procedure{GA}{$W$, $R$, $T$}\Comment{$W$ and $R$ are a set of workflow and resources respectively. $T$ is a dictionary of when a resource becomes available.}
        \State \texttt{Initaliaze population}
        \While{Conergence Criteria not met and \#Gen $<$ Total\_Generations}
        \State{Selection}
        \State{Reproduce}
        \State{Randomly mutate}
        \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The members of the initial population are constructed either randomly or semi-randomly.
Specifically, a percentage of the workflows are assigned randomly to resources, where the assignment is drawn by uniform distribution.
The rest of the workflows are assigned based on an earlier finish time (EFT) heuristic, similar to the one used by HEFT.
The population size is relatively small to 20 members, i.e., potential plans, called micro-GA.
Micro-GA reduces the computational load of the genetic algorithm, while it does not significantly impact the final result~\cite{zomaya2001observations}.

The genetic algorithm uses a fitness function to calculate the goodness of a population member or plan.
This function returns a value between 0 and 1 for all plans in the population.
The fitness function calculates the distance of a plan from the ideal makespan.
This distance is defined as:
\begin{equation}
E = \sqrt{(\sum_{j=1}^{N_{C}}TTX_{w_{j},r_{j}} - IM)^2}
\label{eq:fitness}
\end{equation}
where $N_{C}$ is the number of workflows and $IM$ is the ideal makespan.
The ideal makespan is equal to
\begin{equation}
IM = \frac{\sum_{i=1}^{N_{C}}T_{w_{i}}}{\sum_{j=1}^{N_{R}}r_{j}}
\label{eq:ideal_fitness}
\end{equation}
The fitness of a plan is the equal to $F = 1 /E$ when $E > 0$, otherwise it is equal to 1.

Selection of the members to reproduce is based on their fitness values.
Each member fitness value defines the probability of that member to be selected.
Reproduction uses cyclic rotation to generate the new population members.
Mutation randomly selects a member from the population and randomly swaps workflows.

The genetic algorithm has the following algorithmic characteristics:
\begin{inparaenum}[1)]
    \item creates plans in a non-deterministic method,
    \item produces an expectation when resources are available and
\end{inparaenum}

HEFT initially calculates the average execution time of a workflow on all resources and creates a priority list with the longest workflow first.
In addition, HEFT initializes a vector where each element is when the respective resource is available.
Then, it pulls the first workflow from the priority list and calculates when it will finish at each resource.
Finally, it places the workflow on the resource that will finish it earlier and updates the resource availability.
HEFT ends when all workflows are placed on a resource.

% -------------------------------------------------------------------
\subsection{Longest to Fastest First Available Resource Algorithm}
\label{algo:l2ff}
The last algorithm places workflows based on the rule largest workflows to fastest resource~\cite{balasubramanian2019programming}.
This algorithm sorts the workflows based on the number of operations and resources based on their performance.
Then it places each workflow on the first fastest available resource.
Algorithm~\ref{alg:l2ff} show the pseudocode for this algorithm.

\begin{algorithm}[ht]
    \caption{Longest to Fastest First (L2FF)}
    \label{alg:l2ff}
    \begin{algorithmic}[1]
        \Procedure{l2ff}{$W$, $R$}\Comment{$W$ and $R$ are a set of workflow and resources respectively.}
        \State \texttt{$W_{sorted}=sort(W)$} 
        \State \texttt{$R_{sorted}=sort(R)$}
        \For{$w$ in $W_{sorted}$}
        \State{Assign $w$ to $r_{k}$ where $k=w_{idx} \mod N_{R}$}
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

% -------------------------------------------------------------------
\section{Performance Evaluation of Planning Algorithms}
\label{sec:algo_perf_comp}

We executed three experiments to evaluate and analyze the performance of the selected algorithms.
The first experiment measures the makespan with a plan that uses HEFT, GA, L2FF and Random for different campaign and resource sizes in static resources.
The second experiment measures the sensitivity of the makespan to resource dynamism, i.e. resource performance changes over time.
We define sensitivity as the difference between the makespan measured via executing a campaign and the planned makespan.
Last, we measure the sensitivity of the makespan to workflow runtime uncertainty.
These sets of experiments provide a methodology to compare planning algorithms and decide which algorithm is more suited based on a campaign's computational requirements, i.e. workflow size, resource performance/availability and initial information uncertainty.

During the execution of our experiments, we are making a set of assumptions.
These assumptions are based on the supported use cases requirements and high performance computing resources we have access to.
During our experiments we either vary the campaign size and keep the number of resources constant or vary the number of resources and keep the campaign size constant.
The campaign size varies from 4 to 2048 workflows which satisfies requirement 1 from Table~\ref{tab:fun_reqs}.
The number of resources vary from 4 up to 256 resources as per requirement 3 from Table~\ref{tab:fun_reqs}.

Resources can be homogeneous or heterogeneous and static or dynamic.
Resources are considered homogeneous when they have the same performance in terms of operations per second, otherwise they are heterogeneous.
Further, static resources are those whose performance does not change over runtime and dynamic when the resource performance changes over runtime.
When resources are homogeneous we assume that their performance is equal to 1 Peta-flop.
We use four existing HPC resources as the basis for generating heterogeneous resources.
These resources are PSC Bridges, SDSC Comet, TACC Stampede2 and TACC Frontera with peak performance at 1.3, 2.7, 10.6 and 23.5 PetaFLOPS respectively.

Workflow execution runtime is based on an ecological use case and specifically the one described in \S~\ref{ch:designs}.
The runtime of the use case is defined by a mean value of 75000 seconds and a variance of 6000 seconds when executed to a resource with performance of 1 PETAFLOP.
For the purpose of our experiments, we assume that workflow runtimes are drawn by a normal distribution with a mean and variance based on this use case.

\subsection{Experiment 1: Measuring makespan on static resources}

In our first experiment, we measure the makespan using HEFT, GA, L2FF and random for different campaign and resource sizes for static resources.
We either vary the campaign size from 4 to 2048 workflows while keeping the number of resources constant to 4, or vary the number of resources from 4 to 256 while keeping the number of workflows equal to 1024.

First, we measure the makespan of exeuting a homogeneous campaign, workflows with runtime of 75000 seconds,  on homogeneous resources with performance of 1 PETAFLOP.
In this case an algorithm produces the minimum makespan when it is able to equally distribute the workflows of the campaign to the available resources.
The genetic algorithm allows us to select the percentage of the workflows assigned non-randomly for each individual of the population.
We used 0~\% (GA in the figure), 25~\% (GA-25) and 50~\% (GA-50).
In addition, the genetic algorithm stops when it converges, i.e., an individual plan has a makespan equal to the ideal makespan as defined in Eq.~\ref{eq:ideal_fitness}, or after 100 iterations and returns the individual in the current population with the best fitness value.

The percentage of non-random placement affects the plan the genetic algorithm produces.
Figure~\ref{fig:ga_conv1} shows the convergence rate of the genetic algorithm as the number of workflows changes while the number of resources is constant.
When the population is initialized randomly, GA does not produce always a plan with the ideal makespan.
The converge rate of GA is always less than one even.
GA-25 convergence rate shows some small drop for 8, 16 and 32 workflows, but the value is less than 0.03 which is not enough to conclude that GA-25 does not produce plans with the ideal makespan.
GA-50 consistently produces plans with the ideal makespan when the number of workflows varied.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHomoCampaigns_4StHomoResourcesGAconv.pdf}
        \caption{}
        \label{fig:ga_conv1}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/HomogeResources_StHomogeCampaignsGAconv.pdf}
        \caption{}
        \label{fig:ga_conv2}
    \end{subfigure}
    \caption{Convergence rate of Genetic algorithm for homogeneous campaigns on static homogeneous resources based on random initialization percentage: ~\ref{fig:ga_conv1}) Different campaign sizes on 4 resources;~\ref{fig:ga_conv2}) Campaign with 1024 workflows and different number of resources.}
    \label{fig:conv_rate}
\end{figure*}

Figure~\ref{fig:ga_conv2} shows the convergence rate of the genetic algorithm as the number of resources changes while the number of workflows is constant.
When the population is initiallized randomly, the genetic algorithm does not produce a plan that has the ideal makespan.
The convergence rate of GA-25 is 1 up to 16 resource and drops to 0 for more than 32 resources.
GA-50 convergence rate is 1 up to 64 resources and drops to less than 0.2 for 128 resources and equal to 0 for 256 resources.

The analysis of the convergence rate of the genetic algorithm for different initial configurations provide us with information about the produced plans.
We expect that GA will not provide on average a plan with the ideal makespan whether the campaign size changes or the number of resources.
GA-25 will provide plans with the ideal makespan when the number of resources is less than 16 regardless the number of workflows.
GA-50 provides plans with ideal makespan up to 64 resources.

Figure~\ref{fig:st_homog_analysis} shows the makespan for a campaign with homogeneous workflows and homogeneous resources a plan which uses HEFT, GA (for the three different configurations), L2FF and random (RA).
Figure~\ref{fig:StHomoCampaigns_4StHomoResources} varies the campaign size from 4 to 2048 workflows while the number of resources is equal to 4 and Figure~\ref{fig:StHomoResources_StHomoCampaigns} varies the number of resources from 4 to 256 resources while the campaign size equals to 1024 workflows.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHomoCampaigns_4StHomoResources.pdf}
        \caption{}
        \label{fig:StHomoCampaigns_4StHomoResources}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHomoResources_StHomoCampaigns.pdf}
        \caption{}
        \label{fig:StHomoResources_StHomoCampaigns}
    \end{subfigure}
    \caption{~\ref{fig:StHomoCampaigns_4StHomoResources} Makespan of increasing number of homogeneous workflows on homogeneous resources.
    ~\ref{fig:StHomoResources_StHomoCampaigns} Makespan of homogeneous campaign on different number of homogeneous resources.}
    \label{fig:st_homog_analysis}
\end{figure}


HEFT and L2FF provide better makespan that Random and the genetic algorithm.
L2FF places equal number of workflows per resource for all available resources and thus produces a plan that offer the minimum makespan.
HEFT places a workflow to the resource that will finish it earlier.
This in turn means that will place a workflow to a resource has less number of workflows compared to the rest and as a result it is able to equally distribute the workflows to the resources.

The genetic algorithm confirms the makespan expectation providing different makespan based on the percentage of workflows that are assigned non-randomly.
The performance of GA is 50~\% better than random and drops to almost 3~\% as the number of workflows increase for all configurations.
When the number of resources changes GA shows 3~\% to 8~\% better performance than random and GA-25 and GA-50 produce up to 25~\% and 50~\% better performance than random.
As a result, we will exclude GA and GA-25 from the next experiments and show only GA-50.

The next run of this experiment shows how resource heterogeneity affects the performance of the algorithms.
All algorithms produce and use different amount of information about resources and specifically resource availability.
Based on the amount and type, absolute or relative, information they produce we expect the algorithms to have different performance with those that use more absolute information to have better performance.


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHomoCampaigns_4StHeteroResources.pdf}
        \caption{}
        \label{fig:StHomoCampaigns_4StHeteroResources}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroResources_StHomoCampaigns.pdf}
        \caption{}
        \label{fig:HeteroResources_StHomoCampaigns}
    \end{subfigure}
    \caption{Makespan of homogeneous worklfows on heterogeneous resources:~\ref{fig:StHomoCampaigns_4StHeteroResources}) varying campaign size with 4 resources;
        ~\ref{fig:HeteroResources_StHomoCampaigns}) varying number of resources for constant campaign size at 1024 workflows.}
    \label{fig:hom_het_analysis}
\end{figure*}

Figure~\ref{fig:hom_het_analysis} shows the makespan of the three algorithms for a campaign with homogeneous workflows on heterogeneous resources.
HEFT provides always better makespan from L2FF and GA-50 whether the campaign size changes (Fig.~\ref{fig:StHomoCampaigns_4StHeteroResources}) or the number of resources (Fig.~\ref{fig:HeteroResources_StHomoCampaigns}).
HEFT produces and uses information of when a resource will be available and uses that information to place a workflow to the resource that will finish it first.
GA-50 produces consistently plans with better makespan that L2FF apart from the case of 256 resources.
The genetic algorithm produces the same information but only after it has placed a subset of the workflows to resources randomly.
The partially random initialization of the genetic algorithm can result to under-utilizing or even not using some resources.
L2FF places the same number of workflows per resource and as a result it is able to use all the resources.
As a result, it has similar performance with random for a large number of workflows compared to the number of resources, but allows it to provide better makespan as the number of resources increase.


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroCampaigns_4StHeteroResources.pdf}
        \caption{}
        \label{fig:StHeteroCampaigns_4StHeteroResources}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroResources_StHeteroCampaigns.pdf}
        \caption{}
        \label{fig:StHeteroResources_StHeteroCampaigns}
    \end{subfigure}
    \caption{~\ref{fig:StHeteroCampaigns_4StHeteroResources}) Makespan of increasing number of heterogeneous worklfows on heterogeneous resources;
        ~\ref{fig:StHeteroResources_StHeteroCampaigns}) Makespan of heterogeneous campaign on different number of heterogeneous resources..}
    \label{fig:heter_analysis}
\end{figure*}

Figure~\ref{fig:heter_analysis} shows the performance of the algorithms when a campaign with heterogeneous workflows executes on heterogeneous resources.
Comparing the results with those in figure~\ref{fig:hom_het_analysis} we do not see significant changes in the performance of the algorithms due to workflow heterogeneity.
Both HEFT and L2FF similarly rank the workflows.
HEFT calculates the average execution time between resources, while L2FF sorts the workflows.
Although GA does not rank workflows, it places part of the workflows on resources that will finish them the earlier.
As a result, resource heterogeneity affects the performance of the algorithms more than workflow heterogeneity.

In summary, we can conclude that resource heterogeneity dominates the makespan performance among the considered algorithms.
On homogeneous resources, HEFT, GA-50 and L2FF plans produce similar makespan.
On heterogeneous resources, HEFT makespan is at least an order of magnitude smaller than L2FF makespan and is more than two times smaller than GA-50 makespan independent of workflow homogeneity and heterogeneity.
Finally, the characteristics of the selected algorithms that affect the makespan performance are:
\begin{inparaenum}[1)]
    \item estimating when a resource will be available;
    \item deterministic or randomized heuristics; and
    \item creating a workflow priority list based on workflow runtime
\end{inparaenum}

\subsection{Experiment 2: Measuring makespan sensitivity on dynamic resources}

High performance computing resource performance is affected by a number of factors.
The shared filesystem and network performance, for example, changes based on I/O patterns and network congestion~\cite{brown2018interference}.
The performance of the CPUs and GPU of individual nodes are subject to policies that may allow multiple users on the same node, change CPU clock frequencies based on temperature and more.
As a result, the performance of a resource during the execution of a workflow can be different from the one a planning algorithm is expecting and thus dynamic.

The second experiments measures how sensitive is the plan produced by each algorithm to resource dynamicity.
We assume that the dynamic performance is drawn by a normal distribution where the mean performance is defined by the performance of the specific resource when it is considered static.
The variance is such that $3\sigma$ from the mean almost 20\% away from the mean.
Figure~\ref{fig:dynamic_res} shows an example of the distribution for mean equals to 1 and sigma equals to 0.06.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.95\textwidth]{figures/campaign/DynRes.pdf}
    \caption{Resource performance distribution of a dynamic resource with 1 Peta-Flops average performance.}
    \label{fig:dynamic_res}
\end{figure}

We executed with the same configurations as in Experiment~1. 
Figure~\ref{fig:dyn_hetero_analysis} shows the performance of each algorithm when workflows are heterogeneous and resources are heterogeneous and dynamic.
Comparing with Figure~\ref{fig:het_het_analysis} we understand that all makespans are affected by resource dynamicity.
One important note is that HEFT due to its deterministic behavior and knowledge about resource availability still offers the better makespan compared to the other two algorithms.
A random plan provides still the worst makespan.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroCampaigns_4DynHeteroResources.pdf}
        \caption{}
        \label{fig:StHeteroCampaigns_4DyHeteroResources}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/campaign/DynHeteroResources_StHeteroCampaigns.pdf}
        \caption{}
        \label{fig:DyHeteroResources_StHeteroCampaigns}
    \end{subfigure}
    \caption{~\ref{fig:StHeteroCampaigns_4DyHeteroResources} Makespan of increasing number of homogeneous workflows on homogeneous resources;
        ~\ref{fig:DyHeteroResources_StHeteroCampaigns} Makespan of homogeneous campaign on different number of homogeneous resources.}
    \label{fig:dyn_hetero_analysis}
\end{figure}

Figures~\ref{fig:dyn_homog_sens_analysis}, ~\ref{fig:dyn_hetero_homog_sens_analysis} and~\ref{fig:dyn_hetero_sens_analysis} show the results of our experiments for homogeneous workflows/ homogeneous resources, heterogeneous workflows/homogeneous resources and heterogeneous workflows/ heterogeneous resources.
In all configurations, we varied the number of workflows and the number of resources in the same way as experiment 1.

Figures~\ref{fig:StHomoCampaigns_4DyHomoResourcesSens} and~\ref{fig:DyHomoResources_StHomoCampaignsSens} show how sensitive are the algorithms when workflows and resources are homogeneous.
The random plan shows the least sensitivity as for both cases, varying the varying the number of workflows (Fig.~\ref{fig:StHomoCampaigns_4DyHomoResourcesSens} and varying the number of resources (Fig.~\ref{fig:DyHomoResources_StHomoCampaignsSens}).
All three algorithms are affected the same way by the introduced dynamicity.
One important note is that the variability of the makespan is not more than 10\% in all cases.
The algorithmic sensitivity decreases as the number of workflows increase when the number of resources is constant.
In addition, it increases as the number of resources increase when the number of workflows remains constant.
This points us to the conclusion that the sensitivity of the algorithms is proportional to the ratio of workflows over resources.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHomoCampaigns_4DynHomoResourcesSens.pdf}
        \caption{}
        \label{fig:StHomoCampaigns_4DyHomoResourcesSens}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/campaign/DynHomoResources_StHomoCampaignsSens.pdf}
        \caption{}
        \label{fig:DyHomoResources_StHomoCampaignsSens}
    \end{subfigure}
    \caption{~\ref{fig:StHomoCampaigns_4DyHomoResourcesSens} Normalized makespan sensitivity of increasing number of homogeneous workflows on homogeneous resources;
        ~\ref{fig:DyHomoResources_StHomoCampaignsSens} Normalized makespan sensitivity  of homogeneous campaign on different number of homogeneous resources.}
    \label{fig:dyn_homog_sens_analysis}
\end{figure}

In figure~\ref{fig:dyn_hetero_homog_sens_analysis} we introduce workflow heterogeneity.
The algorithms makespan performance behavior is similar to the one shown in Fig.~\ref{fig:het_het_analysis}.
Despite the fact, HEFT show the largest sensitivity when varying the number of workflows and when varying the number of resources.
The genetic algorithm shows similar sensitivity with HEFT when the number of workflows but tends to be less sensitive when the number of resources increase.
The genetic algorithm assigns a percentage of workflows to resources randomly which in turn makes it less sensitive to resource dynamicity, especially when the number of resources is large.

L2FF shows a similar behavior with HEFT, especially when the number of resources changes, despite being less sensitive.
In addition, the genetic algorithm sensitivity gets closer to that of random when the number of resources increases.
Both L2FF and HEFT are deterministic and they produce always the same plan for a given campaign and set of resources.
This in turn results to an increasing sensitivity as the ratio of number of campaign size to number of resources is small.
In contrast, the random initial placement of GA allows it to create a plan that is less sensitive to dynamicity with the cost of slightly makespan, 725K seconds versus 643K and 649K seconds from HEFT and L2FF respectively.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroCampaigns_4DynHomoResourcesSens.pdf}
        \caption{}
        \label{fig:StHeteroCampaigns_4DynHomoResourcesSens}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/campaign/DynHomoResources_StHeteroCampaignsSens.pdf}
        \caption{}
        \label{fig:DynHomoResources_StHeteroCampaignsSens}
    \end{subfigure}
    \caption{~\ref{fig:StHeteroCampaigns_4DynHomoResourcesSens} Normalized makespan sensitivity  of increasing number of heterogeneous workflows on homogeneous resources;
        ~\ref{fig:DynHomoResources_StHeteroCampaignsSens} Normalized makespan sensitivity  of a heterogeneous campaign on different number of homogeneous resources.}
    \label{fig:dyn_hetero_homog_sens_analysis}
\end{figure}

Resource heterogeneity has a significant impact in the makespan as seen Experiment 1.
We expect resource dynamicity to reduce this impact.
As shown in Figure~\ref{fig:dyn_hetero_sens_analysis} this is not true.
The makespan is affected less than 10~\% despite the algorithm used.
HEFT shows the largest effect in the makespan.
GA and L2FF show similar impact with the random placement, however GA has produces a better plan than both (see Fig.~\ref{fig:dyn_hetero_analysis}), with significant difference when the ratio of campaign size to number of resources is large.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.85\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/StHeteroCampaigns_4DynHeteroResourcesSens.pdf}
        \caption{}
        \label{fig:StHeteroCampaigns_4DyHeteroResourcesSens}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.85\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/campaign/DynHeteroResources_StHeteroCampaignsSens.pdf}
        \caption{}
        \label{fig:DyHeteroResources_StHeteroCampaignsSens}
    \end{subfigure}
    \caption{~\ref{fig:StHeteroCampaigns_4DyHeteroResourcesSens} Normalized makespan sensitivity  of increasing number of heterogeneous workflows on heterogeneous resources;
    ~\ref{fig:DyHeteroResources_StHeteroCampaignsSens} Normalized makespan sensitivity  of heterogeneous campaign on different number of heterogeneous resources.}
    \label{fig:dyn_hetero_sens_analysis}
\end{figure}

This experiment measured the impact of resource dynamicity on the makespan produced by the selected algorithms.
Although resource dynamicity did not change the ranking of the algorithms, it shows that algorithms with strong assumptions about workflow number of operations, resource performance and resource availability are affected the most.
Furthermore, the impact of resource dynamicity is inverse proportional to the ratio of campaign size over number of resources.
Based on the assumptions an algorithm makes, resource heterogeneity can affect the impact dynamic resource may have on the makespan of the produced plan.
This experiment together with experiment 1 allow us to conclude that resource heterogeneity is the factor that affects the most the performance of the algorithms.


\subsection{Experiment 3: Measuring makespan sensitivity based workflow length uncertainty}

All of the selected algorithms, apart from the random, require information about number of operations or length of each workflow.
The user usually offers this kind of information either through a function or a single number drawn by empirical data.
When executing computational campaigns that process complex datasets, it is understandable that even the most accurate information can have some level of uncertainty.
This, in turn, means that execution plans may not produce the best possible makespan.
This experiment measures how sensitive are the plans produced by the selected algorithms for different levels of uncertainty.

We introduce workflow information uncertainty as the difference between the estimated and actual runtime of a workflow on a 1 Peta-flops resource.
Specifically, we denote as $u$ the level of uncertainty between $[0,1)$, $u'$ the uncertainty for a workflow drawn randomly from the range $[-u,u]$ and $w$ the estimated runtime of a workflow.
So the actual runtime of the workflow $w'$ is $ w' = w \times (1-u')$.

As shown in the previous experiments the algorithms show significant performance differences when heterogeneous resources are used.
As a result, we executed this experiment for heterogeneous workflows and heterogeneous static and dynamic resources for different levels of uncertainty.
We use normalized sensitivity as the selected metric for this experiment and measured it for uncertainty from 10~\% up to 90~\% with 10~\% step.
Figure~\ref{fig:inaccur_st} shows the results for static resources with figure~\ref{fig:InaccurStHeteroCampaigns_4StHeteroResourcesSens} varying the number of workflows and~\ref{fig:InaccurStHeteroResources_StHeteroCampaignsSens} varying the number of resources.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.85\textwidth}
        \includegraphics[width=.95\textwidth]{figures/campaign/InaccurStHeteroCampaigns_4StHeteroResourcesSens.pdf}
        \caption{}
        \label{fig:InaccurStHeteroCampaigns_4StHeteroResourcesSens}
    \end{subfigure}\\
    ~ 
    \begin{subfigure}[b]{0.85\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/campaign/InaccurStHeteroResources_StHeteroCampaignsSens.pdf}
        \caption{}
        \label{fig:InaccurStHeteroResources_StHeteroCampaignsSens}
    \end{subfigure}
    \caption{~\ref{fig:InaccurStHeteroCampaigns_4StHeteroResourcesSens} Makespan sensitivity for different levels of uncertainty and different number of workflows on static resources;
    ~\ref{fig:InaccurStHeteroResources_StHeteroCampaignsSens} Makespan sensitivity for different levels of uncertainty and different number of resources on static resources.}
    \label{fig:inaccur_st}
\end{figure}

The ratio of number of workflows over the number of resources remains inverse proportional to the sensitivity.
In addition, specifically for HEFT the impact of uncertainty increases significantly as the number of resources increase, reaching 55~\%.
Since HEFT places one or two workflows on these resources, any change affects significantly its expected makespan.
This verifies our conclusion from experiment 2, that algorithms with strong assumptions about workflow runtime and resource availability are very sensitive to changes.

%\begin{figure}[ht!]
%   \centering
%    \begin{subfigure}[b]{0.95\textwidth}
%        \includegraphics[width=.95\textwidth]{figures/campaign/InaccurStHeteroCampaigns_4DynHeteroResourcesSens.pdf}
%        \caption{}
%        \label{fig:InaccurStHeteroCampaigns_4DynHeteroResourcesSens}
%    \end{subfigure}\\
%    ~ 
%    \begin{subfigure}[b]{0.95\textwidth}
%        \includegraphics[width=0.95\textwidth]{figures/campaign/InaccurStHeteroResources_StHeteroCampaignsSens.pdf}
%        \caption{}
%        \label{fig:InaccurDynHeteroResources_StHeteroCampaignsSens}
%    \end{subfigure}
%    \caption{~\ref{fig:InaccurStHeteroCampaigns_4DynHeteroResourcesSens} Makespan sensitivity for different levels of uncertainty and different number of workflows on static resources;
%    ~\ref{fig:InaccurDynHeteroResources_StHeteroCampaignsSens} Makespan sensitivity for different levels of uncertainty and different number of resources on static resources.}
%    \label{fig:inaccur_dyn}
%\end{figure}

Introducing resource dynamism increased further the sensitivity of the algorithms, as seen in figure~\ref{fig:inaccur_dyn}.
The observed increase was no more than the level shown in experiment 2.
This shows us that the overall sensitivity is the summation of independently measured sensitivity from different sources.

The results of our experiments show that algorithms which make significant assumptions about the workflow length and resource performance produce better makespan.
That is apparent from the fact that HEFT consistently produces the smaller makespan.
This is true also when resources are dynamic and the workflows runtime is uncertain even at 90~\%.
HEFT had a sensitivity of around 60~\% maximum which is not enough to affect the plan in such a way that another algorithm would be preferable.
Furthermore, knowledge of resource heterogeneity and availability is an important factor in differentiating the performance of the algorithms, as seen by the performance difference of L2FF compared to HEFT and GA.
Lastly, the ratio of the number of workflows over the number of resources affects the level of sensitivity of the algorithms.
The less number of workflows placed on resources higher the significance of a change either in the performance of the resources or the workflow runtime.
%\subsubsection{Time to calculate a plan}
%
%Based on experiments HEFT and L2FF will have the smallest overhead for calculating the makespan.
%This is due to their deterministic behavior.
%GA on the other hand is a non-deterministic algorithm and its runtime depends on its convergence rate.


\section{Selecting Planning Algorithms and Campaign Budgeting Framework}
\label{sec:cf_algo_sel}
Based on our analysis, we conclude two things.
First, that any type of planning will offer a better makespan than random placement.
Only L2FF had the same makespan as the random plan when heterogeneous resources were introduced.
Second, that algorithms which create knowledge about when in time a resource is available and are deterministic in nature produce better makespan.
Both HEFT and GA produce such knowledge and consistently produced better makespan from L2FF or random.
Further, GA introduces random placement of workflows on resource to initialize its learning procedure and a result it either underutilized or did not use at some resources.

Through the above experimental analysis we can derive a empirical framework to calculate the budget, in terms of time or core hours, required to successfully execute a computation campaign.
This is especially important for campaigns whose objective is to finish in a given amount of time or with a specific budget, as well as when users plan to execute a campaign and want to calculate their budget.
Based on this experimental methodology, users can perform a best (certain workflow runtime/ static resources), worst (high workflow runtime uncertainty/ dynamic resources) and average (workflow runtime uncertainty/ dynamic resources) case analysis.
Assuming that the makespan produced by an offline planning algorithm in the best case scenario is $TTX_{C}(M)$, the makespan sensitivity due to workflow runtime uncertainty and resource dynamism is $TTX_{S}(M)$ and performance gains from adapting the plan is $TTX_{G}(M)$.
Calculating the value of $TTX'_{C}(M) = TTX_{C}(M) \times (1 + TTX_{S}(M))\times ( 1 - TTX_{G}(M))$ provides an estimate of the necessary amount of time to safely execute a campaign.
Such information can then easily change to a monetary value of cost of resource usage and allow users to either request enough resources to execute their campaign or verify whether they have enough resources.

%% --------------------------------- OLD TEXT ----------------------------------

%% ---------------------------- HEFT EXTENSION ---------------------------------
%HPC resources may become unavailable for multiple reasons, including but not limited to maintenance, a random failure, executing a workflows over the expected time., etc.
%In order to be able to utilize HEFT for dynamic resources, we had to extend Algorithm~\ref{alg:heft} to take as input the time that a resource is initially available.
%This input can be represented as a dictionary where the keys are the available resources and the values are the time a resource becomes available.
%The extended algorithm is shown in Algorithm~\ref{alg:ext_heft}.
%Although the extension may be small, it is crucial to allow to reuse HEFT based on the state of the execution at a given point in time.
%
%\begin{algorithm}[ht]
%    \caption{Extended Heterogeneous Earliest Finish Time (EHEFT) algorithm}
%    \label{alg:ext_heft}
%    \begin{algorithmic}[1]
%        \Procedure{EHEFT}{$W$, $R$, $T$}\Comment{$W$ and $R$ are a set of workflow and resources respectively. $T$ is a dictionary of when a resource becomes available.}
%        \State \texttt{Calculate the computation cost $w_{tx}^{ij}$ of each workflow for all resources}
%        \State \texttt{Assign $rank_i = \overline{w_{i}} = \nicefrac{\sum_{j=1}^{|R|}w_{tx}^{ij}}{|R|}$}
%        \State \texttt{Sort workflows by non-increasing order of $rank_i$}
%        \While{unscheduled workflows}
%        \State \texttt{Select the first workflow $\tilde{w}$ from the sorted list}
%        \For{$\forall r_{j}$ in $R$}
%        \State\texttt{Compute earliest finish time for $\tilde{w}$ on $r_{j}$ based on $T(r_j)$, $eft_{\tilde{w},r_j}$ }
%        \EndFor
%        \State \texttt{Assign  $\tilde{w}$ on $r_k$ with $\min{(eft_{\tilde{w},r_j})}$}
%        \EndWhile
%        \EndProcedure
%    \end{algorithmic}
%\end{algorithm}
%

%% -------------------- GA EXTENSION -------------------------------------------
%We extend this algorithm in the population initialization to support replanning..
%The initialization method takes into account the times resources will be available during for the EFT heuristic.
%Another point of extension would be the fitness function.
%However, the fitness function of the selected algorithm~\cite{page2005algorithm} already takes into account the previous load of a resource, which is the time a resource is available.

%% -----------------------------------------------------------------------------
%
%\subsection{Experiment 4: Performance Gain using Plan Adaptation}
%
%An adaptive plan is a new plan derived from the previous one on the base of information acquired at runtime.
%Plan adaptation can happen either via running the planning algorithm again with the new information or via submitting a workflow to the selected resource when it becomes available.
%In this experiment, we focus on the second type of adaptation since L2FF does not know when exactly a resource becomes available.
%Specifically, when a resource becomes available earlier than expected, the bookkeeping component pushes the next workflow to the selected resource, otherwise the workflow is queued and executes as soon the resource becomes available.
%
%We measure the performance gain of adaptive plans for executing a campaign with heterogeneous workflows and different level of uncertainty on heterogeneous dynamic resources.
%Figure~\ref{fig:gain_dyn} shows the normalized performance gain as a function of the workflow runtime uncertainty and either the number of workflows, fig.~\ref{fig:InaccurStHeteroCampaigns_4DynHeteroResourcesGain}, or the number of resources, fig.~\ref{fig:InaccurDynHeteroResources_StHeteroCampaignsGain}.
%As we can see the performance gain is not significant.
%L2FF adaptive plan shows a maximum of 16~\% for 4 workflows and 4 resources, GA and HEFT around 12~\%, while when the number of resources vary none of the plans exceeds 6~\%.
%
%
%\begin{figure}[ht!]
%    \centering
%    \begin{subfigure}[b]{0.95\textwidth}
%        \includegraphics[width=.95\textwidth]{figures/campaign/InaccurStHeteroCampaigns_4DynHeteroResourcesGain.pdf}
%        \caption{}
%        \label{fig:InaccurStHeteroCampaigns_4DynHeteroResourcesGain}
%    \end{subfigure}\\
%    ~ 
%    \begin{subfigure}[b]{0.95\textwidth}
%        \includegraphics[width=0.95\textwidth]{figures/campaign/InaccurDynHeteroResources_StHeteroCampaignsGain.pdf}
%        \caption{}
%        \label{fig:InaccurDynHeteroResources_StHeteroCampaignsGain}
%    \end{subfigure}
%    \caption{~\ref{fig:InaccurStHeteroCampaigns_4DynHeteroResourcesGain} Normalized performance gain for different levels of uncertainty and different number of workflows on static resources;
%        ~\ref{fig:InaccurDynHeteroResources_StHeteroCampaignsGain} Normalized performance gain for different levels of uncertainty and different number of resources on static resources.}
%    \label{fig:gain_dyn}
%\end{figure}
%
%Since adaptive plans try to mitigate the effects of resource dynamism and workflow runtime uncertainty, we expect that the performance gain would be proportional to the ratio of number of workflows over the number of resources.
%Although this is generally true for L2FF and GA, it is not for HEFT.
%This is due to the fact that HEFT is placing a small number, 1 or 2, of workflows on slow resources.
%As a result, adaptive plans have no or very little effect.
%Contrary, L2FF and GA place more workflows on the slow resources allowing the adaptation to provide some gains.
