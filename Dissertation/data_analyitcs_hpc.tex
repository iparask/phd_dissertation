% !TEX root = main.tex
\label{ch:data_hpc}
%Frameworks for parallel data analysis have been created by the High Performance Computing (HPC) and Big Data communities~\cite{kamburugamuve2017anatomy}.
%MPI is the most used programming model for HPC resources.
%It assumes a SPMD execution model where each process executes the same program.
%It is highly optimized for high-performance computing and communication, which along with synchronization need explicit implementation.
%Big Data frameworks utilize higher-level MapReduce~\cite{dean2004mapreduce} programming models avoiding explicit implementations of communication.
%In addition, the MapReduce~\cite{dean2004mapreduce} abstraction makes it easy to exploit data-parallelism as required by many analysis workflows.

Frameworks for parallel data analysis have been created by the High Performance Computing (HPC) and Big Data communities~\cite{kamburugamuve2017anatomy}.
Until now MPI is the most used programming model.
It assumes a SPMD execution model where each process executes the same program.
It is highly optimized for high-performance computing and communication, which along with synchronization need explicit implementation.
Big Data frameworks utilize higher-level MapReduce~\cite{dean2004mapreduce} programming models avoiding explicit implementations of communication.
In addition, the MapReduce~\cite{dean2004mapreduce} abstraction makes it easy to exploit data-parallelism as required by many analysis workflows.

Task-parallel applications involve partitioning a workload into a set of self-contained units of work.
Based on the application, these tasks can be independent, have no inter-task communication, or coupled with varying degrees of data dependencies.
Big Data applications exploit task parallelism for data-parallel operations (e.\,g., \texttt{map}), but also require coupling, for computing aggregates (\texttt{reduce}).
The MapReduce~\cite{dean2004mapreduce} abstraction popularized this execution pattern.
Typically, a reduce operation includes shuffling intermediate data from a set of nodes to node(s) where the reduce executes.

Spark~\cite{zaharia2010spark} and Dask~\cite{rocklin2015dask} are two Big Data frameworks.
Both provide MapReduce abstractions and are optimized for parallel processing of large data volumes, interactive analytics and machine learning.
Their runtime engines can automatically partition data, generate parallel tasks, and execute them on a cluster.
In addition, Spark offers in-memory capabilities allowing caching data that are read multiple times, making it suited for interactive analytics and iterative machine learning algorithms.
Dask also provides a MapReduce API (Dask Bags).
Furthermore, Dask's API is more versatile, allowing custom workflows and parallel vector/matrix computations.

In \S~\ref{sec:pilot-data-hadoop}, we explore the integration between Hadoop~\cite{hadoop}, and Spark~\cite{zaharia2010spark} and HPC resources.
We utilize the Pilot-Abstraction~\cite{luckow2012pstar} allowing the application to manage HPC and data-intensive application stages in a uniform way.
We explore two extensions to RADICAL-Pilot~\cite{merzky2018design}, a Pilot-Job~\cite{luckow2012pstar} runtime system designed for implementing task-parallel applications on HPC: 
\begin{inparaenum}[(i)]
    \item the ability to spawn and manage Hadoop/Spark clusters on HPC infrastructures on demand (Mode I),
    \item and to connect and utilize Hadoop and Spark clusters for HPC applications (Mode II)
\end{inparaenum}.
Both extensions facilitate the complex application and resource management requirements of data-intensive applications.
By supporting these two usage modes, RADICAL-Pilot dramatically simplifies the barrier of deploying and executing HPC and Hadoop/Spark side-by-side.

In \S~\ref{sec:task-par}, we investigate three task-parallel frameworks, Spark, Dask and RADICAL-Pilot, and their suitability for implementing MD trajectory analysis algorithms.
We also utilize MPI4py~\cite{dalcin2005mpi} to provide MPI equivalent implementations of the algorithms.
The task-parallel implementations performance and scalability compared to MPI is the basis of our analysis.
MD trajectories are time series of atoms/particles positions and velocities, which are analyzed using different statistical methods to infer certain properties, e.\,g. the relationship between distinct trajectories, snapshots of a trajectory etc.
As a result, they can be considered as a representative set of scientific datasets that are organized as time series and their analysis algorithms. 
The section makes the following contributions: 
\begin{inparaenum}[i)]
    \item it characterizes and explains the behavior of different MDAnalysis algorithms on these frameworks, and
    \item provides a conceptual basis for comparing the abstraction, capabilities and performance of these frameworks.
\end{inparaenum}


\section{Integrating Hadoop and Spark with HPC workload management system}
\input{pilot_data_hadoop}


\section{Modeling the task-parallel execution of data intensive workflows}
\input{task_par}