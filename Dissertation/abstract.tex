% !TEX root = main.tex
Computational sciences such as biomolecular 
sciences~\cite{cheatham2015impact, dakka2018concurrent}, ecological 
sciences~\cite{goncalves2020sealnet, paraskevakos2019workflow} and physics 
depending on large scale experiments~\cite{atlas} are benefiting from 
executing multiple workflows, with or without dependencies amongst them, to 
achieve scientific insight. Users submit multiple workflows for execution and
monitor their execution while achieving a computational objective. This way
of execution is called a computational campaign.

This dissertation addresses the problem of efficiently and effectively 
executing a computational campaign on High Performance Computing
(HPC) resources. Specifically, the dissertation focuses on computational 
campaigns with data-driven workflows utilizing HPC resources.

The MapReduce abstraction is used successfully to execute data-intensive 
workflows and applications on cloud resources. However, MapReduce frameworks 
are not well supported on HPC resources. We propose the extension of the 
Pilot-Abstraction to provide a unifying resource management layer between HPC 
and MapReduce frameworks. MapReduce frameworks utilize task-parallelism to 
execute workflows in a scalable and efficient manner. However, different 
frameworks, though, offer different abstractions and capabilities and not one 
framework can support all types of data-intensive workflows. Thus, we 
experimentally investigate the suitability of task-parallel frameworks for the 
execution of data-intensive analytics on HPC. This experimental analysis 
provides the information to produce a conceptual model as to which framework is 
more suitable based on the characteristics of the data analysis application.

In addition to MapReduce style workflows, there are data analysis workflows 
that do not necessarily conform to MapReduce. These workflows are data-driven 
and compute-intensive requiring efficient data management and utilization of 
heterogeneous resources. From the middleware perspective, there are 
architecturally equivalent task-parallel designs to support such workflows. We 
experimentally characterize such designs and show which design approach is best 
suited for scientific workflows with similar characteristics.

Having established the methodology to effectively and efficiently execute
data-driven workflows on HPC, we design and implement a campaign manager
prototype. The campaign manager prototype creates an execution plan and
executes the campaign. Further, we investigate three algorithms to derive an
execution plan and characterize their performance in terms of makespan and
plan sensitivity to resource performance changes and workflow runtime
estimation uncertainty. Based on our analysis, we provide a conceptual model
for selecting a suitable planning algorithm based on characteristics of a
computational campaign and resources.

The research of this dissertation makes the following contributions:
\begin{inparaenum}
    \item Integrate MapReduce frameworks and HPC to offer unified environment 
    for compute and data intensive applications
    \item Provide a conceptual model for selecting a task-parallel framework 
    based on the application requirements and framework abstractions and 
    performance
    \item Provide design guidelines for supporting data-based compute-intensive 
    workflows on HPC and an experimental methodology to compare equivalent 
    designs
    \item Design and implement a campaign manager prototype to plan and execute 
    computational campaigns
    \item Provide a conceptual model for selecting a planning algorithm based 
    on campaign, resources and algorithm characteristics
    
\end{inparaenum}
