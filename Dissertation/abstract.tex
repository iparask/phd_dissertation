% !TEX root = main.tex
Computational sciences such as biomolecular sciences~\cite{cheatham2015impact,
dakka2018concurrent}, ecological sciences~\cite{goncalves2020sealnet,
paraskevakos2019workflow} and particle physics~\cite{atlas} execute multiple
workflows to achieve scientific insight. Users submit multiple workflows for
execution and monitor their execution while achieving a computational objective.
This way of execution multiple workflows is called a computational campaign.

This dissertation addresses the problem of effectively and efficiently executing
a computational campaign on High Performance Computing (HPC) resources.
Specifically, the dissertation focuses on computational campaigns with
data-intensive workflows that utilize HPC resources at scale.

Data-intensive workflows are not well supported on HPC resources. Contrary, the
MapReduce abstraction is used successfully to execute data-intensive workflows
on cloud resources. We utilize the Pilot-Abstraction as an integrating concept
between HPC and MapReduce frameworks (e.g., Hadoop). We extend RADICAL-Pilot, a
Pilot-Abstraction framework, to support Hadoop on HPC resources and
experimentally characterize the execution time of data-intensive workflows and
extension's overheads.

MapReduce frameworks utilize task-parallelism to execute workflows in a
scalable and efficient manner, offer different abstractions and capabilities.
We experimentally investigate the suitability of three task-parallel
frameworks for the execution of data-intensive workflows on HPC resources.
Based on our experimental analysis, we develop a conceptual model as to which
framework is more suitable based on the characteristics of the data-intensive
workflows.

In addition to MapReduce style workflows, there are data analysis workflows
that do not necessarily conform to MapReduce. These workflows are data-
and compute-intensive requiring efficient utilization of heterogeneous 
resources. From the middleware perspective, there are architecturally 
equivalent task-parallel designs to support such workflows. We implement and  
experimentally characterize three equivalent designs and show which design 
approach is best suited for scientific workflows with similar characteristics.

Having established the methodology to effectively and efficiently execute
data-driven workflows on HPC, we design and implement a campaign manager
prototype. The campaign manager prototype creates an execution plan and
executes the campaign. Further, we investigate three algorithms to derive an
execution plan and characterize their performance in terms of makespan and
plan sensitivity to resource performance changes and workflow runtime
estimation uncertainty. Based on our analysis, we provide a conceptual model
for selecting a suitable planning algorithm based on characteristics of a
computational campaign and resources.

The research of this dissertation makes the following contributions:
\begin{inparaenum}
    \item Integrate MapReduce frameworks and HPC to offer unified environment
    for compute and data intensive applications
    \item Provide a conceptual model for selecting a task-parallel framework
    based on the application requirements and framework abstractions and
    performance
    \item Provide design guidelines for supporting data-based compute-intensive
    workflows on HPC and an experimental methodology to compare equivalent
    designs
    \item Design and implement a campaign manager prototype to plan and execute
    computational campaigns
    \item Provide a conceptual model for selecting a planning algorithm based
    on campaign, resources and algorithm characteristics

\end{inparaenum}
