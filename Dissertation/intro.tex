% !TEX root = main.tex
\gpnote{\label{ch:intro}
\begin{itemize}
    \item Describe use cases and explain why they are computational campaigns.
    \item Show that campaigns can be simulation based, analysis based, and simulation analysis based
    \item RADICAL-EnTK and RP can be used to run simulation based campaigns
    \item They lack data-analytics based capabilities
    \item Question 1: How can these tools offer data analytics capabilities?
    \item Question 2: How does that affect the performance and how does EnTK and RP perform?
    \item Question 3: Is there a design choice that can affect the execution?
    \item Question 4: What is needed to fully support campaigns?
\end{itemize}}

Scientific applications are benefiting form executing multiple workflows, with of without dependencies amongst them, to achieve scientific insight.
These applications span from biomolecular sciences~\cite{dakka2018concurrent}, to enable drug discovery, to data analysis from large scale experiments~\cite{atlas} or remote sensing~\cite{goncalves2020sealnet}.
Furthermore, the characteristics of the workflows that comprise such applications are different.
Others gather data either from simulators, such as the ATLAS experiment~\cite{atlas}, or remote sensing, such as satellites~\cite{goncalves2020sealnet}, and require to analyze those data.
Last but not least are the applications that execute workflows that are simulating systems, analyze produced data and then steer further simulations~\cite{}.
This way of execution is called computational campaign.

Some drug discovery applications, for example, screen millions of compounds to identify good drug candidates~\cite{dakka2018concurrent}.
Each compound screening eventually is a workflow which computes the binding affinity of the compound.
In addition, such applications utilize High Performance Computing (HPC) resources and consume millions of core-hours of compute time.
In order to maximize the number of good candidates a large number of such workflows should be executed.

Another example of computational campaigns is quantum chemistry campaigns supported by QCFractal~\cite{qcfractal}.
These campaigns usually have hundreds of workflows as members and can be executed on a set of different resources, based on workflow requirements.
Moreover, users want to have the ability to prioritize some workflows over others and remove or add workflows or resources.

Ecological sciences that utilize very high resolution satellite imagery~\cite{goncalves2020sealnet}, require the analysis of TB of data from different calendar years to create time series of ecological changes.
To achieve this, ecological scientist would like to analyze data as they are becoming available by satellite imagery providers. 


As it is evident from the above use cases, computational campaigns have a plethora of characteristics and requirements.
Some of the most important characteristics of campaigns are:
\begin{inparaenum}[(1)]
    \item comprised of O(100) workflows;
    \item workflow requirements are not uniform across workflows;
    \item use of multiple computational resources;
    \item finite access time to said resources; and
    \item well defined computational objectives.
\end{inparaenum}

Based on the above discussion, it becomes important to support the execution of computational campaigns as the first order of execution.
Thus there is a need to provide software stack that allows the execution of compute and data oriented workflows on a diverse set of resources.
Currently, there is a stack comprised by RADICAL-EnTK~\cite{balasubramanian2018harnessing}, RADICAL-Pilot~\cite{merzky2019using} and RADICAL-SAGA~\cite{merzky2015saga} which allows the execution of compute intensive workflows on HPC resources.
Although, compute oriented workflows and workloads are fully supported, there is a lack of support of data oriented workflows.
As a result, supporting data oriented and data intensive workflows becomes important, as well as understanding understanding the performance characteristics of data-intensive workflows.

%\subsection*{Paper 1 intro}

%The MapReduce~\cite{mapreduce} abstraction popularized by Apache Hadoop~\cite{hadoop} has been successfully used for many data-intensive applications in different domains~\cite{37684}.
%One important differentiator of Hadoop compared to HPC is the availability of many higher-level abstractions and tools for data storage, transformations and advanced analytics.
%These abstraction typically allow high-level reasoning about data parallelism without the need to manually partition data, manage tasks processing this data and collecting the results, which is required in other environments.
%Within the Hadoop ecosystem, tools like Spark~\cite{Zaharia:2010:SCC:1863103.1863113} have gained popularity by supporting specific data processing and analytics needs and are increasingly used in sciences, e.\,g.\ for DNA sequencing~\cite{Massie:EECS-2013-207}.


%Data-intensive applications are associated with a wide variety of characteristics and properties, as summarized by Fox et\,al.~\cite{bigdata-ogres,bigdata-use-cases-nist}.
%Their complexity and characteristics are fairly distinct from HPC applications.
%For example, they often comprise of multiple stages such as, data ingest, pre-processing, feature-extraction and advanced analytics.
%While some of these stages are I/O bound, often with different patterns (random/sequential access), other stages are compute-/memory-bound.
%Not surprisingly, a diverse set of tools for data processing (e.\,g.\ MapReduce, Spark RDDs), access to data sources (streaming, filesystems) and data formats (scientific data formats (HDF5), columnar formats (ORC and Parquet)) have emerged and they often need to be combined in order to support the end-to-end needs of applications.

Data-intensive workflows and applications are associated with a wide variety of characteristics and properties, as summarized by Fox et\,al.~\cite{fox2014towards,fox2014big}.
Their complexity and characteristics are fairly distinct from compute intensive applications.
They often comprise of multiple stages such as, data ingest, pre-processing, feature-extraction and advanced analytics.
While some of these stages are I/O bound, often with different patterns (random/sequential access), other stages are compute-/memory-bound.
As a result, a diverse set of tools for data processing (e.\,g.\ MapReduce~\cite{dean2004mapreduce}, Spark~\cite{zaharia2010spark}), access to data sources and data formats have emerged and often need to be combined in order to support the end-to-end needs of applications.

%Some applications however, defy easy classification as data-intensive or HPC.
%In fact, there is specific interest in a class of of scientific applications, such as bio-molecular dynamics~\cite{doi:10.1146/annurev-biophys-042910-155245}, that have strong characteristics of both data-intensive and HPC.
%Bio-molecular simulations are now high-performant, reach increasing time scales and problem sizes, and thus generating immense amounts of data.
%The bulk of the data in such simulations is typically trajectory data that is time-ordered set of coordinates and velocity.
%Secondary data includes other physical parameters including different energy components.
%Often times the data generated needs to be analyzed so as to determine the next set of simulation configurations.
%The type of analysis varies from computing the higher order moments, to principal components, to time-dependent variations.

Some applications however, defy easy classification as compute- or data-intensive.
In fact, there is a class of scientific applications, such as bio-molecular dynamics~\cite{dror2012biomolecular}, that have strong characteristics of both compute- and data-intensive.
Bio-molecular simulations are now highly performant, reaching larger problem sizes, and generating immense amounts of data.
Often, the data generated need to be analyzed and determine the next set of simulation workflows.

%MDAnalysis~\cite{mdanalysis} and CPPTraj~\cite{doi:10.1021/ct400341p} are two tools that evolved to meet the increasing analytics demands of molecular dynamics applications; Ref~\cite{himach} represents an attempt to provide MapReduce based solutions in HPC environments.
%These tools provide powerful domain-specific analytics; a challenge is the need to scale them to high data volumes produced by molecular simulations as well as the coupling between the simulation and analytics parts.
%This points to the need for environments that support scalable data processing while preserving the ability to run simulations at the scale so as to generate the data.
%To the best of our knowledge, there does not exist a solution that provides the integrated capabilities of Hadoop and HPC.
%For example, Cray's analytics platform Urika~\footnote{http://www.cray.com/products/analytics} has Hadoop and Spark running on HPC architecture as opposed to regular clusters, but without the HPC software environment and capabilities.
%However, several applications ranging from bio-molecular simulations to epidemiology models~\cite{network1} require significant simulations interwoven with analysis capabilities such as clustering and graph analytics; in other words some stages (or parts of the same stage) of an application would ideally utilize Hadoop/Spark environments and other stages (or parts thereof) utilize HPC environments.

MDAnalysis~\cite{michaud2011mdanalysis} and CPPTraj~\cite{roe2013ptraj} are two tools that evolved to meet the increasing analytics demands of molecular dynamics applications; 
Tools like HiMach~\cite{tu2008scalable} represent an attempt to provide MapReduce based solutions in HPC environments.
These tools support powerful domain-specific data analysis workflows.
A challenge is the need to scale them to high data volumes as well as couple them with simulation workflows~\cite{balasubramanian2016extasy}.
To the best of our knowledge, there is no runtime system that provides the integrated capabilities of compute- and data- intensive workflows and applications.

%Over the past decades, the High Performance Distributed Computing (HPDC) community has made significant advances in addressing resource and workload management on heterogeneous resources.
%For example, the concept of multi-level scheduling~\cite{1392910} as manifested in the decoupling of workload assignment from resource management using the concept of intermediate container jobs (also referred to as Pilot-Jobs~\cite{pstar12}) has been adopted for both HPC and Hadoop.
%Multi-level scheduling is a critical capability for data-intensive applications as often only application-level schedulers can be aware of the localities of the data sources used by a specific application.
%This motivated the extension of the Pilot-Abstraction to Pilot-Data~\cite{pilot-data-jpdc-2014} to form the central component of a resource management middleware.

Over the past decades, the HPC community has made significant advances in addressing resource and workload management on heterogeneous resources.
For example, the concept of multi-level scheduling~\cite{berman1996application} as manifested in decoupling workload placement from resource management using intermediate container jobs (also known as Pilot-Jobs~\cite{luckow2012pstar}) has been adopted for both HPC and Hadoop.
Multi-level scheduling is a critical capability for data-intensive applications, as often only application-level schedulers are aware of data locality of the data used by an application.

%In this paper, we explore the integration between Hadoop and HPC resources utilizing the Pilot-Abstraction allowing application to manage HPC (e.\,g.\ simulations) and data-intensive application stages in a uniform way.
%We propose two extensions to RADICAL-Pilot: the ability to spawn and manage Hadoop/Spark clusters on HPC infrastructures on demand (Mode I), and to connect and utilize Hadoop and Spark clusters for HPC applications (Mode II).
%Both extensions facilitate the complex application and resource management requirements of data-intensive applications that are best met by a best-of-bread mix of Hadoop and HPC.
%By supporting these two usage modes, RADICAL-Pilot dramatically simplifies the barrier of deploying and executing HPC and Hadoop/Spark side-by-side.

In Chapter~\ref{sec:pilot-data-hadoop}, we explore the integration between Hadoop and HPC resources utilizing the Pilot-Abstraction allowing application to manage HPC and data-intensive application stages in a uniform way.
We propose two extensions to RADICAL-Pilot: the ability to spawn and manage Hadoop/Spark clusters on HPC infrastructures on demand (Mode I), and to connect and utilize Hadoop and Spark clusters for HPC applications (Mode II).
Both extensions facilitate the complex application and resource management requirements of data-intensive applications.
By supporting these two usage modes, RADICAL-Pilot dramatically simplifies the barrier of deploying and executing HPC and Hadoop/Spark side-by-side.


%\subsection*{Paper 2 intro}
%Frameworks for parallel data analysis have been created by the High Performance Computing (HPC) and Big Data communities~\cite{fox-2017}.
%MPI is the most used programming model for HPC resources.
%It assumes a SPMD execution model where each process executes the same program.
%It is highly optimized for high-performance computing and communication, which along with synchronization need explicit implementation.
%Big Data frameworks utilize higher-level MapReduce~\cite{mapreduce} programming models avoiding explicit implementations of communication.
%In addition, the MapReduce~\cite{mapreduce} abstraction makes it easy to exploit data-parallelism as required by many analysis applications.
%Several recent publications applied HPC techniques to advance traditional Big Data applications and Big Data frameworks~\cite{fox-2017}.

Frameworks for parallel data analysis have been created by the High Performance Computing (HPC) and Big Data communities~\cite{kamburugamuve2017anatomy}.
MPI is the most used programming model for HPC resources.
It assumes a SPMD execution model where each process executes the same program.
It is highly optimized for high-performance computing and communication, which along with synchronization need explicit implementation.
Big Data frameworks utilize higher-level MapReduce~\cite{dean2004mapreduce} programming models avoiding explicit implementations of communication.
In addition, the MapReduce~\cite{dean2004mapreduce} abstraction makes it easy to exploit data-parallelism as required by many analysis workflows.

%Task-parallel applications involve partitioning a workload into a set of self-contained units of work.
%Based on the application, these tasks can be independent, have no inter-task communication, or coupled with varying degrees of data dependencies.
%Big Data applications exploit task parallelism for data-parallel parts (e.\,g., \texttt{map} operations), but also require coupling, for computing aggregates (the \texttt{reduce} operation).
%The MapReduce~\cite{mapreduce} abstraction popularized this execution pattern.
%Typically, a reduce operation includes shuffling intermediate data from a set of nodes to node(s) where the reduce executes.
%There is a recognized need to optimize communication intensive parts of Big Data frameworks using established HPC techniques for interprocess, e.\,g. shuffle operations~\cite{rdma-spark} and other forms of communication~\cite{hpc-abds,twisterNet}.

Task-parallel applications involve partitioning a workload into a set of self-contained units of work.
Based on the application, these tasks can be independent, have no inter-task communication, or coupled with varying degrees of data dependencies.
Big Data applications exploit task parallelism for data-parallel parts (e.\,g., \texttt{map} operations), but also require coupling, for computing aggregates (the \texttt{reduce} operation).
The MapReduce~\cite{dean2004mapreduce} abstraction popularized this execution pattern.
Typically, a reduce operation includes shuffling intermediate data from a set of nodes to node(s) where the reduce executes.

%Spark~\cite{zaharia2010spark} and Dask~\cite{rocklin2015dask} are two Big Data frameworks.
%Both provide MapReduce abstractions and are optimized for parallel processing of large data volumes, interactive analytics and machine learning.
%Their runtime engines can automatically partition data, generate parallel tasks, and execute them on a cluster.
%In addition, Spark offers in-memory capabilities allowing caching data that are read multiple times, making it suited for interactive analytics and iterative machine learning algorithms.
%Dask also provides a MapReduce API (Dask Bags).
%Furthermore, Dask's API is more versatile, allowing custom workflows and parallel vector/matrix computations.

Spark~\cite{zaharia2010spark} and Dask~\cite{rocklin2015dask} are two Big Data frameworks.
Both provide MapReduce abstractions and are optimized for parallel processing of large data volumes, interactive analytics and machine learning.
Their runtime engines can automatically partition data, generate parallel tasks, and execute them on a cluster.
In addition, Spark offers in-memory capabilities allowing caching data that are read multiple times, making it suited for interactive analytics and iterative machine learning algorithms.
Dask also provides a MapReduce API (Dask Bags).
Furthermore, Dask's API is more versatile, allowing custom workflows and parallel vector/matrix computations.

%In this paper, we investigate the data analysis requirements of Molecular Dynamics (MD) applications.
%MD simulations are significant consumers of computing cycles, producing immense amounts of data.
%A typical $\mu sec$ MD simulation of physical system of $O(100k)$ atoms can produce from $O(10)$ to $O(1000)$ GBs of data~\cite{cheatham2015impact}.
%In addition to being the prototypical HPC application, there is increasingly a need for the analysis to be integrated with simulations and drive the next stages of execution~\cite{balasubramanian2016extasy}.
%The analysis phase must be performed quickly and efficiently in order to steer the simulations.

%We investigate three task-parallel frameworks and their suitability for implementing MD trajectory analysis algorithms.
%In addition to Spark and Dask, we investigate RADICAL-Pilot~\cite{rp-jsspp18}, a Pilot-Job~\cite{pstar12} framework designed for implementing task-parallel applications on HPC.
%We utilize MPI4py~\cite{mpi4py_paper} to provide MPI equivalent implementations of the algorithms.
%The task-parallel implementations performance and scalability compared to MPI is the basis of our analysis.
%MD trajectories are time series of atoms/particles positions and velocities, which are analyzed using different statistical methods to infer certain properties, e.\,g. the relationship between distinct trajectories, snapshots of a trajectory etc.
%As a result, they can be considered as a representative set of scientific datasets that are organized as time series and their analysis algorithms. 

In Chapter~\ref{task-par}, we investigate three task-parallel frameworks and their suitability for implementing MD trajectory analysis algorithms.
In addition to Spark and Dask, we investigate RADICAL-Pilot~\cite{merzky2018design}, a Pilot-Job~\cite{luckow2012pstar} runtime system designed for implementing task-parallel applications on HPC.
We utilize MPI4py~\cite{dalcin2005mpi} to provide MPI equivalent implementations of the algorithms.
The task-parallel implementations performance and scalability compared to MPI is the basis of our analysis.
MD trajectories are time series of atoms/particles positions and velocities, which are analyzed using different statistical methods to infer certain properties, e.\,g. the relationship between distinct trajectories, snapshots of a trajectory etc.
As a result, they can be considered as a representative set of scientific datasets that are organized as time series and their analysis algorithms. 

%\subsection*{Paper 3 intro}
%A growing number of scientific domains are adopting workflows that use multiple analysis algorithms to process a large number of images.
%The volume and scale of data processing justifies the use of parallelism, tailored programming models and high performance computing (HPC) resources.
%While these features create a large design space, the lack of architectural and performance analyses makes it difficult to chose among functionally equivalent implementations.

A growing number of scientific domains are adopting workflows that use multiple analysis algorithms to process large datasets.
The volume and scale of data processing justifies the use of parallelism, tailored programming models and high performance computing (HPC) resources.
While these features create a large design space, the lack of architectural and performance analyses makes it difficult to chose among functionally equivalent implementations.

%In this paper we focus on the design of computing frameworks that support the execution of heterogeneous tasks on HPC resources to process large imagery datasets.
%These tasks may require one or more CPUs and GPUs, implement diverse functionalities and execute for different amounts of time.
%Typically, tasks have data dependencies and are therefore organized into workflows.
%Due to task heterogeneity, executing workflows poses the challenges of effective scheduling, correct resource binding and efficient data management.
%HPC infrastructures exacerbate these challenges by privileging the execution of single, long-running jobs.

In Chapter~\ref{designs} we focus on the design of computing frameworks that support the execution of heterogeneous tasks on HPC resources to process large imagery datasets.
These tasks may require heterogeneous resources, implement diverse functionalities and execute for different amounts of time.
Typically, tasks have data dependencies and are therefore organized into workflows.
%Due to task heterogeneity, executing workflows poses the challenges of effective scheduling, correct resource binding and efficient data management.
%HPC infrastructures exacerbate these challenges by privileging the execution of single, long-running jobs.

From a design perspective, a promising approach to address these challenges is isolating tasks from execution management.
Tasks are assumed to be self-contained programs which are executed in the operating system (OS) environment of HPC compute nodes.
Programs implement the domain-specific functionalities required by use cases while computing frameworks implement resource acquisition, task scheduling, resource binding, and data management.

Compared to approaches in which tasks are functions or methods, a program-based approach offers several benefits as, for example, simplified implementation of execution management, support of general purpose programming models, and separate programming of management and domain-specific functionalities.
Nonetheless, program-based designs also impose performance limitations, including OS-mediated inter task communication and task spawning overheads, as programs execute as OS processes and do not share a memory space.

Due to their performance limitations, program-based designs of computing frameworks are best suited to execute compute-intense workflows in which each task requires a certain amount of parallelism and runs from several minutes to hours.
The emergence of workflows that require heterogeneous, compute-intense tasks to process large amount of data is pushing the boundaries of program-based designs, especially when scale requirements suggest the use of modern HPC infrastructures with large number of CPUs/GPUs and dedicated data facilities.

Computational campaigns enact an execution plan to allow users to achieve a computational objective under given requirements and constraints.
A computational objective is a set of values selected by the user for a set of metrics, e.g., time to completion and throughput, which can be represented as an objective function.
Requirements describe the minimum amount and type of resources needed to execute each workflow of the campaign, while constraints are the conditions that bound the execution, including but not limited to, resource availability, capacity or costs.
A plan describes a sequence of actions that solves the objective function as, for example, selecting, acquiring and configuring resources, and establishing the execution order of workflows on resources.


Planning and enacting the execution of a campaign poses four main challenges: 
\begin{inparaenum}[(i)]
    \item evaluating the makespan of a campaign on heterogeneous and dynamic resources;
    \item the conditions under which an execution plan performs better compared to a random resource selection;
    \item determining a campaign execution plan on available resources that satisfies the given objective function, requirements and constraints of a campaign, while accounting for resource dynamism; and
    \item adapting the plan in case of deviation from the objective achievement.
\end{inparaenum}

\gpnote{Include list heuristics and genetic algorithms}
There are several methods and algorithms to calculate and optimize the makespan of a workflow~\cite{lu2019review}, including queuing networks~\cite{yao2019throughput,bao2019performance}, domain specific languages~\cite{carothers2017durango,maheshwari2016workflow}, and machine learning~\cite{witt2019predictive,pumma2017runtime}.
Queuing networks will be of limited use because they require from the user to provide a queuing network equivalent to the campaign.
In the case the campaign contains only independent workflows, a single queuing system with multiple servers would be sufficient, but a campaign with complex dependencies between workflows may require expertise outside the user's domain to define the equivalent queuing network.
Domain specific languages approaches either require description of the resource usage of workflows~\cite{carothers2017durango}, or execute part of the campaign to obtain an execution ``skeleton'' of the campaign~\cite{maheshwari2016workflow}.
When executing a campaign, workflows may require days to execute to obtain execution time information, and users rarely know the resource usage of their workflows to provide accurate enough information.
In addition, the workflows of a campaign may be different and executing some of them may not provide any information about the execution of others.

In Chapter~\ref{campaigns}, we investigate algorithms which given a campaign, an objective and a set of constraints, will derive an execution plan.
We define a method to evaluate the performance of planning algorithms.
We then utilize this method to compare a set of planning algorithms.


%Currently, campaign managers are making assumptions about the resources and the middleware they are utilizing, are monolithic software systems, and tend to be domain specific.
%The CM we propose to prototype will avoid these three limitations. 
%Our CM will support multiple use case from different domains, such as molecular dynamics and earth sciences.
%As a result, it will be domain agnostic.
%In addition, our CM will also be designed by following the building blocks approach~\cite{turilli2019middleware}. 
%In this way, it will be agnostic of the system used to manage the execution of the campaign workflows.
%This, in turn, will allow our CM to make no assumptions about the resources on which the campaign workflows will be mapped and executed.

Currently, campaign managers are making assumptions about the resources and the middleware they are utilizing, are monolithic software systems, and tend to be domain specific.
Thus, they are domain gnostic.
In contrast, a software system designed and engineered following the building blocks approach~\cite{turilli2019middleware} is agnostic of the middleware used to execute the campaign.
In addition, it is domain agnostic.
This, in turn, will allow the campaign manager to make no assumptions about the resources on which the campaign workflows will be mapped and executed.

In Chapter~\ref{cmanager}, we design a campaign manager (CM) which, given a campaign, an objective, and a set of constraints, can derive an execution plan and execute a campaign.
If necessary the CM will change the execution plan by updating workflow to resource mapping, and resource availability.
Execution planning for workflows are provided by several workflow execution systems, such as Pegasus~\cite{deelman2015pegasus}, and ASKALON~\cite{fahringer2005askalon}.
Campaign management systems, such as PanDA~\cite{maeno2008panda}, do not provide a campaign planning feature.
QCFractal~\cite{qcfractal} offers some form of planning by allowing user to specify the priority of a workflow in a campaign, but this planning does not take into account the makespan of the campaign.

